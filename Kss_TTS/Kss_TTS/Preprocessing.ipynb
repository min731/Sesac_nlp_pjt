{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1pGP_9a6uSiydSoZIhM9iqA6X_TAcc07Z","authorship_tag":"ABX9TyPPScDeH3Tm25k3KZ57pqMd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d2a6b88c0b464c7cbac7a6d9fbc2d807":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5c1bb8b89db4ce8b33c16c3a5ee15b8","IPY_MODEL_9abe292f9fae419c8b24819fd3c7c205","IPY_MODEL_b5d440caa99d4fd5841f94f243ab229c"],"layout":"IPY_MODEL_46eec2c0f1db4fca9cdc7517645ce3fb"}},"f5c1bb8b89db4ce8b33c16c3a5ee15b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae60c8885b4045eeb3f4ce51e2c91008","placeholder":"​","style":"IPY_MODEL_236782f1e4c8434fb7bae22a5cc2a515","value":"100%"}},"9abe292f9fae419c8b24819fd3c7c205":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce2fa68fee84d189c2c18a59ea780bd","max":101,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b20af07e6fa44ceab6226d207826b9e6","value":101}},"b5d440caa99d4fd5841f94f243ab229c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75867d3b5a6247fdae22a24f83b44a47","placeholder":"​","style":"IPY_MODEL_0bfcb2d6353a49e88766cd0f91d03d79","value":" 101/101 [00:28&lt;00:00,  3.04it/s]"}},"46eec2c0f1db4fca9cdc7517645ce3fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae60c8885b4045eeb3f4ce51e2c91008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"236782f1e4c8434fb7bae22a5cc2a515":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ce2fa68fee84d189c2c18a59ea780bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b20af07e6fa44ceab6226d207826b9e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75867d3b5a6247fdae22a24f83b44a47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bfcb2d6353a49e88766cd0f91d03d79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 타코트론2 TTS 시스템\n","\n","##https://joungheekim.github.io/2021/04/01/code-review/"],"metadata":{"id":"v3qUf0I6Phja"}},{"cell_type":"markdown","source":["### Step2.2 공백 자르기 & Sampling Rate 변경\n","\n"],"metadata":{"id":"oBQXTZIOPd_X"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d2a6b88c0b464c7cbac7a6d9fbc2d807","f5c1bb8b89db4ce8b33c16c3a5ee15b8","9abe292f9fae419c8b24819fd3c7c205","b5d440caa99d4fd5841f94f243ab229c","46eec2c0f1db4fca9cdc7517645ce3fb","ae60c8885b4045eeb3f4ce51e2c91008","236782f1e4c8434fb7bae22a5cc2a515","5ce2fa68fee84d189c2c18a59ea780bd","b20af07e6fa44ceab6226d207826b9e6","75867d3b5a6247fdae22a24f83b44a47","0bfcb2d6353a49e88766cd0f91d03d79"]},"id":"awkJx2YCO15f","executionInfo":{"status":"ok","timestamp":1676808022095,"user_tz":-540,"elapsed":29014,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"85a8e577-4d74-4093-d09a-f91eacd91227"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/101 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a6b88c0b464c7cbac7a6d9fbc2d807"}},"metadata":{}}],"source":["## 라이브러리 Import\n","import numpy as np\n","import os\n","from tqdm.notebook import tqdm\n","import librosa\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","import glob\n","import soundfile as sf\n","\n","## 함수 설정\n","\n","## 파일 읽어오기(지정한 Sampling Rate로)\n","def load_audio(file_path, sr=22050):\n","    \"\"\"\n","       file_path : 파일위치\n","       sr : 오디오를 읽을 때 Sampling rate 지정\n","    \"\"\"\n","    ## 확장자 추출\n","    ext = Path(file_path).suffix\n","    \n","    ## 파일 읽기\n","    if ext in ['.wav', '.flac']:\n","        wav, sr = librosa.load(file_path, sr=sr)\n","    elif ext == '.pcm':\n","        wav = np.memmap(file_path, dtype='h', mode='r').astype('float32') / 32767\n","    elif ext in ['.raw', '.RAW']:\n","        wav, sr = sf.read(file_path, channels=1, samlerate=sr, format='RAW', subtype='PCM_16')\n","    else:\n","        raise ValueError(\"Unsupported preprocess method : {0}\".format(ext))\n","        \n","    return wav, sr\n","\n","## 공백 자르기(패딩 추가)\n","def trim_audio(wav, top_db=10, pad_len=4000):\n","    \"\"\"\n","    \n","    \"\"\"\n","    ## 최대 db에 따라 음성의 자를 위치 판별\n","    non_silence_indices = librosa.effects.split(wav, top_db=top_db)\n","    start = non_silence_indices[0][0]\n","    end = non_silence_indices[-1][1]\n","    \n","    ## 음성 자르기\n","    wav = wav[start:end]\n","    \n","    ## padding 추가\n","    wav = np.hstack([np.zeros(pad_len), wav, np.zeros(pad_len)])\n","    \n","    return wav\n","\n","## WAV 그려보기\n","def plot_wav(wav, sr):\n","    ## 그려보기\n","    plt.figure(1)\n","\n","    plot_a = plt.subplot(211)\n","    plot_a.plot(wav)\n","    plot_a.set_xlabel('sample rate * time')\n","    plot_a.set_ylabel('energy')\n","\n","    plot_b = plt.subplot(212)\n","    plot_b.specgram(wav, NFFT=1024, Fs=sr, noverlap=900)\n","    plot_b.set_xlabel('Time')\n","    plot_b.set_ylabel('Frequency')\n","\n","    plt.show()\n","\n","\n","## 시작하기\n","\n","## 타코트론2는 기본적으로 22050 sampling rate에서 동작\n","sampling_rate = 22050\n","## 개인설정에 따라 특정 소리보다 작은 음성을 삭제하도록 설정\n","decibel=10\n","\n","## Wav 파일 읽어오기  pcm 또는 다른 확장자도 사용 가능.\n","## 잡음제거한 파일 dir\n","root_path = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/data/audio_data/2_noise_reduction_data'\n","file_list = glob.glob(os.path.join(root_path, \"*.wav\"))\n","#file_list = glob.glob(os.path.join(root_path, \"*.pcm\"))\n","\n","## 저장할 위치 선택\n","save_path = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/data/audio_data/3_blank_remove_data'\n","os.makedirs(save_path, exist_ok=True)\n","\n","for file_path in tqdm(file_list):\n","    \n","    ## 파일 불러오기(타코트론2는 기본적으로 22050 sampling rate에서 동작)\n","    wav, sr = load_audio(file_path, sr=sampling_rate)\n","    \n","    ## 오디오 자르기(패딩 추가)\n","    trimed_wav= trim_audio(wav, top_db=decibel)\n","    \n","    filename=Path(file_path).name\n","    temp_save_path = os.path.join(save_path, filename)\n","    \n","    ## 저장하기\n","    sf.write(temp_save_path, trimed_wav, sampling_rate)"]},{"cell_type":"code","source":["blank_remove_file_list = glob.glob(os.path.join(root_path, \"*.wav\"))\n","blank_remove_file_list"],"metadata":{"id":"2R6fHTgAQdnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","kss_script_csv = pd.read_csv(\"/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/data/script_data/kss_transcript.v.1.4_2.csv\",encoding='cp949',)"],"metadata":{"id":"WwNQMloEok2T","executionInfo":{"status":"ok","timestamp":1676811775446,"user_tz":-540,"elapsed":1224,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["kss_script_csv.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"id":"KN2rPbsUwKpy","executionInfo":{"status":"ok","timestamp":1676811778795,"user_tz":-540,"elapsed":5,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"542db260-6505-4bbf-fdbf-8ef370a1fc3f"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      file_name                    script1                    script2  \\\n","0  1/1_0000.wav     그는 괜찮은 척하려고 애쓰는 것 같았다.     그는 괜찮은 척하려고 애쓰는 것 같았다.   \n","1  1/1_0001.wav  그녀의 사랑을 얻기 위해 애썼지만 헛수고였다.  그녀의 사랑을 얻기 위해 애썼지만 헛수고였다.   \n","2  1/1_0002.wav                 용돈을 아껴 써라.                 용돈을 아껴 써라.   \n","3  1/1_0003.wav             그는 아내를 많이 아낀다.             그는 아내를 많이 아낀다.   \n","4  1/1_0004.wav               그 애 전화번호 알아?               그 애 전화번호 알아?   \n","\n","                                            tokenize  legnth  \\\n","0    ????? ????????? ????????? ??????? ??? ????????.     3.5   \n","1  ?????? ???????? ????? ???? ?????????? ????????...     4.0   \n","2                               ????????? ???? ????.     1.8   \n","3                       ????? ??????? ????? ???????.     2.3   \n","4                            ?? ?? ?????????? ??????     1.3   \n","\n","                                      eng  \n","0  He seemed to be pretending to be okay.  \n","1        I tried in vain to win her love.  \n","2                 Save your pocket money.  \n","3                  He cherishes his wife.  \n","4                 Do you know his number?  "],"text/html":["\n","  <div id=\"df-1022a3ed-d115-4600-aaf8-8e78dbd594a7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>script1</th>\n","      <th>script2</th>\n","      <th>tokenize</th>\n","      <th>legnth</th>\n","      <th>eng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1/1_0000.wav</td>\n","      <td>그는 괜찮은 척하려고 애쓰는 것 같았다.</td>\n","      <td>그는 괜찮은 척하려고 애쓰는 것 같았다.</td>\n","      <td>????? ????????? ????????? ??????? ??? ????????.</td>\n","      <td>3.5</td>\n","      <td>He seemed to be pretending to be okay.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1/1_0001.wav</td>\n","      <td>그녀의 사랑을 얻기 위해 애썼지만 헛수고였다.</td>\n","      <td>그녀의 사랑을 얻기 위해 애썼지만 헛수고였다.</td>\n","      <td>?????? ???????? ????? ???? ?????????? ????????...</td>\n","      <td>4.0</td>\n","      <td>I tried in vain to win her love.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1/1_0002.wav</td>\n","      <td>용돈을 아껴 써라.</td>\n","      <td>용돈을 아껴 써라.</td>\n","      <td>????????? ???? ????.</td>\n","      <td>1.8</td>\n","      <td>Save your pocket money.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1/1_0003.wav</td>\n","      <td>그는 아내를 많이 아낀다.</td>\n","      <td>그는 아내를 많이 아낀다.</td>\n","      <td>????? ??????? ????? ???????.</td>\n","      <td>2.3</td>\n","      <td>He cherishes his wife.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1/1_0004.wav</td>\n","      <td>그 애 전화번호 알아?</td>\n","      <td>그 애 전화번호 알아?</td>\n","      <td>?? ?? ?????????? ??????</td>\n","      <td>1.3</td>\n","      <td>Do you know his number?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1022a3ed-d115-4600-aaf8-8e78dbd594a7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1022a3ed-d115-4600-aaf8-8e78dbd594a7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1022a3ed-d115-4600-aaf8-8e78dbd594a7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["script_list = list(kss_script_csv['script1'])\n","script_list[0]\n","len(script_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKy_jD7KwesC","executionInfo":{"status":"ok","timestamp":1676812457321,"user_tz":-540,"elapsed":12,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"49ed0027-0dea-414c-982c-6dd72c08d7f5"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12854"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["blank_remove_data_path = \"/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/data/audio_data/3_blank_remove_data\"\n","blank_remove_data_file_list = glob.glob(os.path.join(blank_remove_data_path, \"*.wav\"))\n","blank_remove_data_file_list[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"enA8b8SIxdq8","executionInfo":{"status":"ok","timestamp":1676811920943,"user_tz":-540,"elapsed":8,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"0c420466-be5e-42dc-9904-83de257d9460"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/data/audio_data/3_blank_remove_data/1_0008.wav'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["f = open(r'/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/tacotron2/filelists/jm_train.txt', 'w')\n","for i in range(len(blank_remove_data_file_list)):\n","  print(i)\n","  f.write(blank_remove_data_file_list[i]+\"|\"+script_list[i]+\"\\n\")\n","f.close"],"metadata":{"id":"XOGhMDNSx0Pq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/tacotron2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnNSVqVHzmXw","executionInfo":{"status":"ok","timestamp":1676812687658,"user_tz":-540,"elapsed":8,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"37afea3f-66db-423c-c8df-b5579ff5d866"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/tacotron2\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KSJTGr3W0xM3","executionInfo":{"status":"ok","timestamp":1676812689865,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"19f3a0ec-16f7-41fd-8f8c-33350c206f7d"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/tacotron2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# !pip install jamo\n","# !pip install Unidecode\n","\n","output = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/models'\n","log = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/logs'\n","\n","!python train.py --output_directory=output --log_directory=log --n_gpus=1 --training_files=filelists/jm_train.txt --validation_files=filelists/jm_dev.txt --epochs=500"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QA1Ox13I0x1K","outputId":"0a3bfdd7-bcc1-44f7-f06c-ab2cb37b0d57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FP16 Run: False\n","Dynamic Loss Scaling: True\n","Distributed Run: False\n","cuDNN Enabled: True\n","cuDNN Benchmark: False\n","2023-02-19 13:48:21.198708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-19 13:48:22.093759: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 13:48:22.093901: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-19 13:48:22.093920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Epoch: 0\n","Train loss 0 37.690346 Grad Norm 10.522199 9.63s/it\n","Validation loss 0: 37.849247  \n","Saving model and optimizer state at iteration 0 to output/checkpoint_0\n","Train loss 1 41.561096 Grad Norm 25.001839 1.34s/it\n","Train loss 2 24.007673 Grad Norm 17.977610 1.35s/it\n","Epoch: 1\n","Train loss 3 10.640144 Grad Norm 8.403142 2.11s/it\n","Train loss 4 11.929872 Grad Norm 6.856266 1.28s/it\n","Train loss 5 10.830544 Grad Norm 7.409619 1.30s/it\n","Epoch: 2\n","Train loss 6 8.562754 Grad Norm 7.901887 2.21s/it\n","Train loss 7 12.025093 Grad Norm 15.657898 1.36s/it\n","Train loss 8 11.385775 Grad Norm 27.345924 1.37s/it\n","Epoch: 3\n","Train loss 9 11.771414 Grad Norm 23.032301 1.53s/it\n","Train loss 10 10.705642 Grad Norm 44.565609 1.33s/it\n","Train loss 11 10.970611 Grad Norm 34.403633 1.23s/it\n","Epoch: 4\n","Train loss 12 10.892835 Grad Norm 32.304302 1.43s/it\n","Train loss 13 10.102324 Grad Norm 32.088993 1.32s/it\n","Train loss 14 9.598280 Grad Norm 33.093472 1.35s/it\n","Epoch: 5\n","Train loss 15 9.771399 Grad Norm 31.398520 1.39s/it\n","Train loss 16 9.689528 Grad Norm 24.810583 1.37s/it\n","Train loss 17 6.686951 Grad Norm 52.468620 2.26s/it\n","Epoch: 6\n","Train loss 18 6.546070 Grad Norm 24.989265 2.16s/it\n","Train loss 19 9.176818 Grad Norm 26.414469 1.33s/it\n","Train loss 20 9.220642 Grad Norm 20.364105 1.31s/it\n","Epoch: 7\n","Train loss 21 8.677170 Grad Norm 10.881770 1.48s/it\n","Train loss 22 9.233116 Grad Norm 7.035814 1.30s/it\n","Train loss 23 6.095599 Grad Norm 18.508902 1.95s/it\n","Epoch: 8\n","Train loss 24 8.416060 Grad Norm 8.891565 1.44s/it\n","Train loss 25 6.187472 Grad Norm 10.173094 1.97s/it\n","Train loss 26 8.638264 Grad Norm 13.106707 1.34s/it\n","Epoch: 9\n","Train loss 27 8.787731 Grad Norm 4.833979 1.51s/it\n","Train loss 28 5.865768 Grad Norm 28.696594 1.99s/it\n","Train loss 29 8.240897 Grad Norm 2.879838 1.36s/it\n","Epoch: 10\n","Train loss 30 8.510988 Grad Norm 3.453964 1.52s/it\n","Train loss 31 7.867101 Grad Norm 6.262573 1.37s/it\n","Train loss 32 5.763673 Grad Norm 13.947931 1.95s/it\n","Epoch: 11\n","Train loss 33 8.272258 Grad Norm 8.393708 1.42s/it\n","Train loss 34 8.118161 Grad Norm 3.205468 1.33s/it\n","Train loss 35 6.216588 Grad Norm 21.792690 1.98s/it\n","Epoch: 12\n","Train loss 36 8.559011 Grad Norm 8.395245 1.54s/it\n","Train loss 37 7.818359 Grad Norm 4.341254 1.36s/it\n","Train loss 38 5.654231 Grad Norm 17.011379 1.97s/it\n","Epoch: 13\n","Train loss 39 6.304725 Grad Norm 16.999142 2.16s/it\n","Train loss 40 7.918867 Grad Norm 8.995376 1.30s/it\n","Train loss 41 7.753685 Grad Norm 3.163258 1.35s/it\n","Epoch: 14\n","Train loss 42 7.873840 Grad Norm 5.743050 1.46s/it\n","Train loss 43 7.773848 Grad Norm 7.032641 1.41s/it\n","Train loss 44 5.492437 Grad Norm 15.602779 2.02s/it\n","Epoch: 15\n","Train loss 45 5.340459 Grad Norm 3.863330 2.17s/it\n","Train loss 46 7.185465 Grad Norm 4.057906 1.30s/it\n","Train loss 47 6.641923 Grad Norm 5.297188 1.39s/it\n","Epoch: 16\n","Train loss 48 6.728999 Grad Norm 8.690026 1.46s/it\n","Train loss 49 6.301753 Grad Norm 5.159319 1.37s/it\n","Train loss 50 4.552028 Grad Norm 17.681759 2.02s/it\n","Epoch: 17\n","Train loss 51 5.822040 Grad Norm 10.090709 1.52s/it\n","Train loss 52 5.758677 Grad Norm 7.086296 1.33s/it\n","Train loss 53 3.953329 Grad Norm 18.714695 1.96s/it\n","Epoch: 18\n","Train loss 54 3.958717 Grad Norm 5.936522 2.13s/it\n","Train loss 55 5.237180 Grad Norm 8.993567 1.34s/it\n","Train loss 56 4.777165 Grad Norm 5.437356 1.31s/it\n","Epoch: 19\n","Train loss 57 4.726438 Grad Norm 6.666519 1.62s/it\n","Train loss 58 4.626614 Grad Norm 12.477624 1.49s/it\n","Train loss 59 3.356483 Grad Norm 20.462568 1.96s/it\n","Epoch: 20\n","Train loss 60 5.210194 Grad Norm 26.033743 1.41s/it\n","Train loss 61 3.066133 Grad Norm 4.727038 1.97s/it\n","Train loss 62 3.961991 Grad Norm 7.727160 1.36s/it\n","Epoch: 21\n","Train loss 63 3.294804 Grad Norm 11.244527 2.23s/it\n","Train loss 64 4.097500 Grad Norm 4.726068 1.22s/it\n","Train loss 65 3.803760 Grad Norm 7.139510 1.35s/it\n","Epoch: 22\n","Train loss 66 2.626883 Grad Norm 4.177349 2.15s/it\n","Train loss 67 5.260682 Grad Norm 22.066921 1.31s/it\n","Train loss 68 4.456479 Grad Norm 14.821522 1.35s/it\n","Epoch: 23\n","Train loss 69 2.531386 Grad Norm 13.624725 2.13s/it\n","Train loss 70 3.909594 Grad Norm 7.501466 1.32s/it\n","Train loss 71 3.825752 Grad Norm 8.544228 1.37s/it\n","Epoch: 24\n","Train loss 72 2.554917 Grad Norm 2.472195 2.12s/it\n","Train loss 73 4.267880 Grad Norm 11.415446 1.32s/it\n","Train loss 74 3.830957 Grad Norm 4.423669 1.02s/it\n","Epoch: 25\n","Train loss 75 3.248009 Grad Norm 5.150637 1.48s/it\n","Train loss 76 3.802816 Grad Norm 6.052737 1.31s/it\n","Train loss 77 2.480279 Grad Norm 5.234425 1.95s/it\n","Epoch: 26\n","Train loss 78 3.389161 Grad Norm 6.726772 1.51s/it\n","Train loss 79 2.489366 Grad Norm 5.594060 1.98s/it\n","Train loss 80 3.425312 Grad Norm 4.939070 1.34s/it\n","Epoch: 27\n","Train loss 81 3.320770 Grad Norm 2.957817 1.42s/it\n","Train loss 82 2.504915 Grad Norm 7.780031 1.97s/it\n","Train loss 83 3.366146 Grad Norm 4.717694 1.32s/it\n","Epoch: 28\n","Train loss 84 3.501954 Grad Norm 4.682031 1.51s/it\n","Train loss 85 3.294069 Grad Norm 2.440686 1.19s/it\n","Train loss 86 2.371758 Grad Norm 3.557765 1.95s/it\n","Epoch: 29\n","Train loss 87 3.161224 Grad Norm 10.785650 1.47s/it\n","Train loss 88 3.488349 Grad Norm 6.543552 1.28s/it\n","Train loss 89 2.355977 Grad Norm 4.020452 1.93s/it\n","Epoch: 30\n","Train loss 90 3.576969 Grad Norm 13.147115 1.46s/it\n","Train loss 91 3.415253 Grad Norm 9.136217 1.35s/it\n","Train loss 92 2.412916 Grad Norm 8.663839 1.94s/it\n","Epoch: 31\n","Train loss 93 2.630522 Grad Norm 10.872561 2.12s/it\n","Train loss 94 3.081679 Grad Norm 4.545812 1.30s/it\n","Train loss 95 3.069039 Grad Norm 5.365841 1.35s/it\n","Epoch: 32\n","Train loss 96 3.215535 Grad Norm 2.611638 1.44s/it\n","Train loss 97 2.221935 Grad Norm 8.380985 2.19s/it\n","Train loss 98 3.124524 Grad Norm 5.802322 1.34s/it\n","Epoch: 33\n","Train loss 99 2.226289 Grad Norm 2.953124 2.19s/it\n","Train loss 100 2.990316 Grad Norm 5.006758 1.27s/it\n","Train loss 101 3.107929 Grad Norm 3.619980 1.24s/it\n","Epoch: 34\n","Train loss 102 3.114666 Grad Norm 5.203606 1.40s/it\n","Train loss 103 2.329945 Grad Norm 8.685090 1.98s/it\n","Train loss 104 2.738360 Grad Norm 4.031458 1.35s/it\n","Epoch: 35\n","Train loss 105 3.002090 Grad Norm 4.881207 1.55s/it\n","Train loss 106 2.922138 Grad Norm 5.841583 1.28s/it\n","Train loss 107 2.177774 Grad Norm 7.668489 1.94s/it\n","Epoch: 36\n","Train loss 108 2.768965 Grad Norm 2.735052 1.50s/it\n","Train loss 109 2.858364 Grad Norm 6.934686 1.31s/it\n","Train loss 110 2.950542 Grad Norm 4.076677 1.33s/it\n","Epoch: 37\n","Train loss 111 2.541176 Grad Norm 10.308902 2.17s/it\n","Train loss 112 2.862756 Grad Norm 9.314291 1.38s/it\n","Train loss 113 2.752201 Grad Norm 3.265597 1.33s/it\n","Epoch: 38\n","Train loss 114 1.931188 Grad Norm 3.266744 2.11s/it\n","Train loss 115 2.753869 Grad Norm 7.911011 1.35s/it\n","Train loss 116 2.807104 Grad Norm 3.008844 1.32s/it\n","Epoch: 39\n","Train loss 117 2.713349 Grad Norm 12.371703 2.11s/it\n","Train loss 118 3.261508 Grad Norm 15.627211 1.39s/it\n","Train loss 119 3.111917 Grad Norm 9.264391 1.35s/it\n","Epoch: 40\n","Train loss 120 1.944306 Grad Norm 4.471139 2.22s/it\n","Train loss 121 3.154549 Grad Norm 10.222583 1.30s/it\n","Train loss 122 3.583064 Grad Norm 14.319618 1.29s/it\n","Epoch: 41\n","Train loss 123 2.949136 Grad Norm 7.723451 1.48s/it\n","Train loss 124 2.803123 Grad Norm 2.727143 1.33s/it\n","Train loss 125 2.077280 Grad Norm 7.363870 1.98s/it\n","Epoch: 42\n","Train loss 126 1.966497 Grad Norm 4.490510 2.15s/it\n","Train loss 127 2.764362 Grad Norm 2.331039 1.32s/it\n","Train loss 128 2.868970 Grad Norm 7.889058 1.37s/it\n","Epoch: 43\n","Train loss 129 2.736085 Grad Norm 3.792830 1.44s/it\n","Train loss 130 1.928467 Grad Norm 5.964639 1.94s/it\n","Train loss 131 2.996555 Grad Norm 6.575703 1.34s/it\n","Epoch: 44\n","Train loss 132 2.816711 Grad Norm 2.313066 1.37s/it\n","Train loss 133 2.865668 Grad Norm 5.404473 1.35s/it\n","Train loss 134 1.939041 Grad Norm 4.571426 2.02s/it\n","Epoch: 45\n","Train loss 135 2.582488 Grad Norm 1.660429 1.75s/it\n","Train loss 136 2.050692 Grad Norm 8.365120 1.98s/it\n","Train loss 137 3.115200 Grad Norm 13.337345 1.29s/it\n","Epoch: 46\n","Train loss 138 2.713002 Grad Norm 6.648616 1.44s/it\n","Train loss 139 1.886128 Grad Norm 2.470714 1.98s/it\n","Train loss 140 2.778967 Grad Norm 7.750463 1.40s/it\n","Epoch: 47\n","Train loss 141 1.970750 Grad Norm 3.109245 2.10s/it\n","Train loss 142 2.559634 Grad Norm 3.290491 1.34s/it\n","Train loss 143 2.660424 Grad Norm 3.479428 1.35s/it\n","Epoch: 48\n","Train loss 144 2.651865 Grad Norm 3.211948 1.51s/it\n","Train loss 145 2.521543 Grad Norm 2.825207 1.30s/it\n","Train loss 146 2.553101 Grad Norm 1.880713 1.32s/it\n","Epoch: 49\n","Train loss 147 2.738710 Grad Norm 3.154578 1.45s/it\n","Train loss 148 1.782712 Grad Norm 11.112603 2.00s/it\n","Train loss 149 2.606561 Grad Norm 3.897463 1.29s/it\n","Epoch: 50\n","Train loss 150 1.806795 Grad Norm 2.107315 2.16s/it\n","Train loss 151 2.497235 Grad Norm 4.671459 1.34s/it\n","Train loss 152 2.439945 Grad Norm 2.151346 1.32s/it\n","Epoch: 51\n","Train loss 153 2.698855 Grad Norm 7.661203 1.47s/it\n","Train loss 154 2.434916 Grad Norm 4.541714 1.32s/it\n","Train loss 155 1.924575 Grad Norm 4.907927 1.98s/it\n","Epoch: 52\n","Train loss 156 1.895025 Grad Norm 5.178506 2.18s/it\n","Train loss 157 2.541368 Grad Norm 2.326756 1.31s/it\n","Train loss 158 2.759085 Grad Norm 7.607405 1.29s/it\n","Epoch: 53\n","Train loss 159 2.154404 Grad Norm 9.561403 2.17s/it\n","Train loss 160 2.304671 Grad Norm 4.175860 1.32s/it\n","Train loss 161 2.509231 Grad Norm 5.841165 1.36s/it\n","Epoch: 54\n","Train loss 162 2.597794 Grad Norm 6.517787 1.53s/it\n","Train loss 163 2.434336 Grad Norm 2.038148 1.24s/it\n","Train loss 164 1.805990 Grad Norm 2.481336 1.97s/it\n","Epoch: 55\n","Train loss 165 1.810404 Grad Norm 3.223176 2.12s/it\n","Train loss 166 2.574301 Grad Norm 4.064082 1.38s/it\n","Train loss 167 2.548371 Grad Norm 7.027727 1.29s/it\n","Epoch: 56\n","Train loss 168 2.578787 Grad Norm 7.165997 1.55s/it\n","Train loss 169 2.466803 Grad Norm 2.877998 1.34s/it\n","Train loss 170 1.850817 Grad Norm 3.957756 1.99s/it\n","Epoch: 57\n","Train loss 171 2.484671 Grad Norm 5.908507 1.50s/it\n","Train loss 172 2.341765 Grad Norm 3.306135 1.32s/it\n","Train loss 173 2.598704 Grad Norm 4.631708 1.29s/it\n","Epoch: 58\n","Train loss 174 2.520222 Grad Norm 1.468351 1.40s/it\n","Train loss 175 2.218424 Grad Norm 1.910757 1.38s/it\n","Train loss 176 1.713949 Grad Norm 3.490190 2.01s/it\n","Epoch: 59\n","Train loss 177 2.356939 Grad Norm 1.159123 1.48s/it\n","Train loss 178 2.543064 Grad Norm 3.690566 1.30s/it\n","Train loss 179 1.727043 Grad Norm 2.225662 1.99s/it\n","Epoch: 60\n","Train loss 180 1.658139 Grad Norm 0.997418 2.12s/it\n","Train loss 181 2.452732 Grad Norm 5.475812 1.31s/it\n","Train loss 182 2.387474 Grad Norm 2.768307 1.63s/it\n","Epoch: 61\n","Train loss 183 2.401175 Grad Norm 4.855869 1.51s/it\n","Train loss 184 2.335997 Grad Norm 1.720299 1.29s/it\n","Train loss 185 1.820070 Grad Norm 2.198181 1.93s/it\n","Epoch: 62\n","Train loss 186 1.637356 Grad Norm 2.222009 2.12s/it\n","Train loss 187 2.380190 Grad Norm 1.819190 1.32s/it\n","Train loss 188 2.689315 Grad Norm 3.588453 1.32s/it\n","Epoch: 63\n","Train loss 189 1.689807 Grad Norm 2.381813 2.17s/it\n","Train loss 190 2.751345 Grad Norm 6.258847 1.20s/it\n","Train loss 191 2.597137 Grad Norm 5.924458 1.31s/it\n","Epoch: 64\n","Train loss 192 2.299883 Grad Norm 2.671570 1.47s/it\n","Train loss 193 1.658556 Grad Norm 3.430681 1.96s/it\n","Train loss 194 2.471045 Grad Norm 1.628232 1.31s/it\n","Epoch: 65\n","Train loss 195 2.447292 Grad Norm 2.501693 1.43s/it\n","Train loss 196 1.758827 Grad Norm 2.890701 2.00s/it\n","Train loss 197 2.192104 Grad Norm 2.467379 1.28s/it\n","Epoch: 66\n","Train loss 198 2.338488 Grad Norm 4.436476 1.37s/it\n","Train loss 199 2.305893 Grad Norm 4.643942 1.35s/it\n","Train loss 200 2.637570 Grad Norm 2.353639 1.32s/it\n","Epoch: 67\n","Train loss 201 2.337989 Grad Norm 3.939184 1.49s/it\n","Train loss 202 2.529647 Grad Norm 3.065745 1.29s/it\n","Train loss 203 2.259781 Grad Norm 1.844593 1.32s/it\n","Epoch: 68\n","Train loss 204 2.341396 Grad Norm 1.658957 1.57s/it\n","Train loss 205 1.755823 Grad Norm 4.253845 1.98s/it\n","Train loss 206 2.327207 Grad Norm 4.333244 1.34s/it\n","Epoch: 69\n","Train loss 207 2.234029 Grad Norm 3.094519 1.42s/it\n","Train loss 208 1.793642 Grad Norm 3.646245 1.98s/it\n","Train loss 209 2.415272 Grad Norm 2.931823 1.30s/it\n","Epoch: 70\n","Train loss 210 2.226525 Grad Norm 2.986868 1.54s/it\n","Train loss 211 2.351186 Grad Norm 1.508334 1.31s/it\n","Train loss 212 1.732701 Grad Norm 2.011411 2.01s/it\n","Epoch: 71\n","Train loss 213 1.655257 Grad Norm 2.060861 2.14s/it\n","Train loss 214 2.275434 Grad Norm 1.363573 1.30s/it\n","Train loss 215 2.395045 Grad Norm 4.661339 1.37s/it\n","Epoch: 72\n","Train loss 216 2.321993 Grad Norm 2.875588 1.38s/it\n","Train loss 217 1.698892 Grad Norm 2.927337 2.03s/it\n","Train loss 218 2.433818 Grad Norm 3.480033 1.35s/it\n","Epoch: 73\n","Train loss 219 2.134976 Grad Norm 2.150249 1.40s/it\n","Train loss 220 2.146645 Grad Norm 2.040707 1.34s/it\n","Train loss 221 1.776180 Grad Norm 1.739300 1.96s/it\n","Epoch: 74\n","Train loss 222 1.597541 Grad Norm 1.400104 2.36s/it\n","Train loss 223 2.184677 Grad Norm 2.739358 1.33s/it\n","Train loss 224 2.342347 Grad Norm 1.369245 1.34s/it\n","Epoch: 75\n","Train loss 225 2.273608 Grad Norm 0.879170 1.51s/it\n","Train loss 226 2.224738 Grad Norm 1.195813 1.37s/it\n","Train loss 227 1.600702 Grad Norm 0.957704 1.98s/it\n","Epoch: 76\n","Train loss 228 2.299182 Grad Norm 3.499383 1.42s/it\n","Train loss 229 2.201682 Grad Norm 1.670528 1.30s/it\n","Train loss 230 1.644119 Grad Norm 0.772404 1.96s/it\n","Epoch: 77\n","Train loss 231 2.186419 Grad Norm 1.362956 1.60s/it\n","Train loss 232 1.580072 Grad Norm 1.380914 2.03s/it\n","Train loss 233 2.134506 Grad Norm 1.300671 1.33s/it\n","Epoch: 78\n","Train loss 234 2.320395 Grad Norm 3.738069 1.35s/it\n","Train loss 235 1.627793 Grad Norm 1.320855 1.97s/it\n","Train loss 236 2.378107 Grad Norm 5.158822 1.31s/it\n","Epoch: 79\n","Train loss 237 2.298734 Grad Norm 5.818994 1.43s/it\n","Train loss 238 1.491924 Grad Norm 1.187245 2.04s/it\n","Train loss 239 2.226347 Grad Norm 3.633247 1.36s/it\n","Epoch: 80\n","Train loss 240 2.135250 Grad Norm 1.555775 1.48s/it\n","Train loss 241 2.206330 Grad Norm 3.844126 1.27s/it\n","Train loss 242 1.530383 Grad Norm 1.140180 1.94s/it\n","Epoch: 81\n","Train loss 243 2.286072 Grad Norm 5.434892 1.42s/it\n","Train loss 244 1.619557 Grad Norm 3.623024 1.99s/it\n","Train loss 245 2.111285 Grad Norm 3.477421 1.38s/it\n","Epoch: 82\n","Train loss 246 1.559766 Grad Norm 2.017313 2.09s/it\n","Train loss 247 2.075558 Grad Norm 1.389652 1.31s/it\n","Train loss 248 2.147554 Grad Norm 1.344422 1.27s/it\n","Epoch: 83\n","Train loss 249 2.041207 Grad Norm 1.933928 1.53s/it\n","Train loss 250 2.191036 Grad Norm 1.144826 1.31s/it\n","Train loss 251 1.465655 Grad Norm 1.695107 1.98s/it\n","Epoch: 84\n","Train loss 252 2.084630 Grad Norm 1.786773 1.43s/it\n","Train loss 253 2.214063 Grad Norm 2.655191 1.34s/it\n","Train loss 254 1.407514 Grad Norm 2.012304 1.97s/it\n","Epoch: 85\n","Train loss 255 2.053673 Grad Norm 5.524811 1.50s/it\n","Train loss 256 1.506386 Grad Norm 3.461848 1.94s/it\n","Train loss 257 2.217748 Grad Norm 1.768711 1.31s/it\n","Epoch: 86\n","Train loss 258 2.383580 Grad Norm 3.737063 1.40s/it\n","Train loss 259 2.044958 Grad Norm 1.951453 1.41s/it\n","Train loss 260 2.032342 Grad Norm 2.081545 1.22s/it\n","Epoch: 87\n","Train loss 261 2.059633 Grad Norm 3.563986 1.44s/it\n","Train loss 262 2.131954 Grad Norm 2.386588 1.33s/it\n","Train loss 263 1.576551 Grad Norm 5.204737 2.00s/it\n","Epoch: 88\n","Train loss 264 2.405708 Grad Norm 8.155370 1.41s/it\n","Train loss 265 1.994589 Grad Norm 3.021284 1.34s/it\n","Train loss 266 1.624015 Grad Norm 4.442299 1.99s/it\n","Epoch: 89\n","Train loss 267 2.140385 Grad Norm 7.398046 1.49s/it\n","Train loss 268 1.496267 Grad Norm 2.959957 1.95s/it\n","Train loss 269 2.155529 Grad Norm 4.379717 1.32s/it\n","Epoch: 90\n","Train loss 270 2.289469 Grad Norm 5.975272 1.44s/it\n","Train loss 271 1.983336 Grad Norm 2.519889 1.32s/it\n","Train loss 272 1.964676 Grad Norm 2.604730 1.51s/it\n","Epoch: 91\n","Train loss 273 2.082044 Grad Norm 3.086084 1.32s/it\n","Train loss 274 1.497749 Grad Norm 3.572674 2.04s/it\n","Train loss 275 2.152583 Grad Norm 3.635077 1.36s/it\n","Epoch: 92\n","Train loss 276 2.085203 Grad Norm 3.121370 1.45s/it\n","Train loss 277 1.914547 Grad Norm 2.603862 1.37s/it\n","Train loss 278 2.016284 Grad Norm 2.577898 1.30s/it\n","Epoch: 93\n","Train loss 279 1.369990 Grad Norm 1.993001 2.18s/it\n","Train loss 280 2.018593 Grad Norm 3.745553 1.33s/it\n","Train loss 281 2.035351 Grad Norm 1.839773 1.37s/it\n","Epoch: 94\n","Train loss 282 1.398769 Grad Norm 2.252991 2.15s/it\n","Train loss 283 2.038127 Grad Norm 2.739199 1.35s/it\n","Train loss 284 2.047262 Grad Norm 2.869006 1.20s/it\n","Epoch: 95\n","Train loss 285 2.181159 Grad Norm 5.628597 1.47s/it\n","Train loss 286 1.388023 Grad Norm 1.787554 1.98s/it\n","Train loss 287 1.919596 Grad Norm 1.256752 1.34s/it\n","Epoch: 96\n","Train loss 288 1.476075 Grad Norm 3.545921 2.26s/it\n","Train loss 289 1.808218 Grad Norm 1.202042 1.36s/it\n","Train loss 290 1.933339 Grad Norm 1.280071 1.31s/it\n","Epoch: 97\n","Train loss 291 1.986300 Grad Norm 2.532992 1.34s/it\n","Train loss 292 1.931565 Grad Norm 1.446805 1.31s/it\n","Train loss 293 1.460053 Grad Norm 0.922990 1.98s/it\n","Epoch: 98\n","Train loss 294 1.888709 Grad Norm 1.650643 1.49s/it\n","Train loss 295 2.020145 Grad Norm 4.566353 1.32s/it\n","Train loss 296 1.406080 Grad Norm 2.163942 1.97s/it\n","Epoch: 99\n","Train loss 297 1.887979 Grad Norm 4.257370 1.49s/it\n","Train loss 298 1.353142 Grad Norm 2.010913 1.95s/it\n","Train loss 299 2.052920 Grad Norm 2.623992 1.27s/it\n","Epoch: 100\n","Train loss 300 1.333363 Grad Norm 2.880506 2.12s/it\n","Train loss 301 1.980417 Grad Norm 1.817923 1.40s/it\n","Train loss 302 2.078148 Grad Norm 3.992942 1.31s/it\n","Epoch: 101\n","Train loss 303 2.113785 Grad Norm 2.824718 1.35s/it\n","Train loss 304 1.435686 Grad Norm 1.478373 1.97s/it\n","Train loss 305 1.939573 Grad Norm 4.476647 1.31s/it\n","Epoch: 102\n","Train loss 306 1.423952 Grad Norm 1.961558 2.17s/it\n","Train loss 307 1.875473 Grad Norm 4.073015 1.35s/it\n","Train loss 308 2.050318 Grad Norm 4.151083 1.62s/it\n","Epoch: 103\n","Train loss 309 1.366260 Grad Norm 1.208817 2.21s/it\n","Train loss 310 1.946491 Grad Norm 2.035004 1.33s/it\n","Train loss 311 2.060988 Grad Norm 2.628467 1.20s/it\n","Epoch: 104\n","Train loss 312 1.293988 Grad Norm 2.168774 2.19s/it\n","Train loss 313 2.053145 Grad Norm 4.475945 1.34s/it\n","Train loss 314 1.910197 Grad Norm 2.892579 1.36s/it\n","Epoch: 105\n","Train loss 315 1.913540 Grad Norm 2.896914 1.47s/it\n","Train loss 316 1.368742 Grad Norm 1.931834 1.99s/it\n","Train loss 317 1.903978 Grad Norm 3.519946 1.34s/it\n","Epoch: 106\n","Train loss 318 1.889159 Grad Norm 3.304929 1.41s/it\n","Train loss 319 1.998331 Grad Norm 2.389296 1.31s/it\n","Train loss 320 1.309020 Grad Norm 2.599772 1.96s/it\n","Epoch: 107\n","Train loss 321 1.830665 Grad Norm 2.536855 1.49s/it\n","Train loss 322 1.740615 Grad Norm 1.355010 1.40s/it\n","Train loss 323 1.409597 Grad Norm 2.971758 2.03s/it\n","Epoch: 108\n","Train loss 324 2.017051 Grad Norm 3.681503 1.43s/it\n","Train loss 325 1.843478 Grad Norm 2.317676 1.32s/it\n","Train loss 326 1.267579 Grad Norm 1.325467 1.93s/it\n","Epoch: 109\n","Train loss 327 2.070933 Grad Norm 3.387631 1.29s/it\n","Train loss 328 1.964895 Grad Norm 1.902881 1.32s/it\n","Train loss 329 1.332379 Grad Norm 3.030560 1.99s/it\n","Epoch: 110\n","Train loss 330 1.415851 Grad Norm 2.917012 2.18s/it\n","Train loss 331 1.733888 Grad Norm 3.697891 1.36s/it\n","Train loss 332 1.792045 Grad Norm 1.411546 1.33s/it\n","Epoch: 111\n","Train loss 333 1.783158 Grad Norm 2.038894 1.44s/it\n","Train loss 334 1.807819 Grad Norm 1.171525 1.30s/it\n","Train loss 335 1.338889 Grad Norm 1.409224 2.01s/it\n","Epoch: 112\n","Train loss 336 1.804256 Grad Norm 1.497657 1.35s/it\n","Train loss 337 1.821757 Grad Norm 1.480359 1.35s/it\n","Train loss 338 1.352690 Grad Norm 0.980129 1.95s/it\n","Epoch: 113\n","Train loss 339 1.688769 Grad Norm 2.504130 1.52s/it\n","Train loss 340 1.359912 Grad Norm 1.857872 1.98s/it\n","Train loss 341 1.820454 Grad Norm 3.129473 1.29s/it\n","Epoch: 114\n","Train loss 342 1.279983 Grad Norm 1.973862 2.12s/it\n","Train loss 343 1.912153 Grad Norm 1.917263 1.33s/it\n","Train loss 344 1.768456 Grad Norm 2.044267 1.61s/it\n","Epoch: 115\n","Train loss 345 1.854150 Grad Norm 2.105025 1.48s/it\n","Train loss 346 1.350420 Grad Norm 1.088464 2.01s/it\n","Train loss 347 1.727965 Grad Norm 3.533726 1.32s/it\n","Epoch: 116\n","Train loss 348 1.299863 Grad Norm 1.362656 2.11s/it\n","Train loss 349 1.926618 Grad Norm 3.262851 1.24s/it\n","Train loss 350 1.776517 Grad Norm 1.182914 1.36s/it\n","Epoch: 117\n","Train loss 351 1.315739 Grad Norm 1.906019 2.19s/it\n","Train loss 352 1.769604 Grad Norm 2.282541 1.35s/it\n","Train loss 353 1.702469 Grad Norm 1.256442 1.32s/it\n","Epoch: 118\n","Train loss 354 1.715116 Grad Norm 1.648209 1.43s/it\n","Train loss 355 1.336576 Grad Norm 2.137351 1.92s/it\n","Train loss 356 1.858738 Grad Norm 1.493437 1.29s/it\n","Epoch: 119\n","Train loss 357 1.708853 Grad Norm 1.620846 1.50s/it\n","Train loss 358 1.815019 Grad Norm 1.471872 1.26s/it\n","Train loss 359 1.249117 Grad Norm 1.550733 2.00s/it\n","Epoch: 120\n","Train loss 360 1.325481 Grad Norm 0.896626 2.11s/it\n","Train loss 361 1.645379 Grad Norm 1.437963 1.37s/it\n","Train loss 362 1.832110 Grad Norm 2.350271 1.33s/it\n","Epoch: 121\n","Train loss 363 1.179391 Grad Norm 1.250763 2.17s/it\n","Train loss 364 1.789318 Grad Norm 4.462116 1.40s/it\n","Train loss 365 1.941181 Grad Norm 3.970041 1.26s/it\n","Epoch: 122\n","Train loss 366 1.797692 Grad Norm 2.780019 1.47s/it\n","Train loss 367 1.173782 Grad Norm 1.681157 2.00s/it\n","Train loss 368 1.854251 Grad Norm 2.002343 1.36s/it\n","Epoch: 123\n","Train loss 369 1.701427 Grad Norm 1.650511 1.49s/it\n","Train loss 370 1.225164 Grad Norm 2.923481 2.00s/it\n","Train loss 371 1.801586 Grad Norm 5.270222 1.32s/it\n","Epoch: 124\n","Train loss 372 1.306327 Grad Norm 1.630881 2.17s/it\n","Train loss 373 1.776780 Grad Norm 1.457651 1.26s/it\n","Train loss 374 1.732620 Grad Norm 2.964710 1.29s/it\n","Epoch: 125\n","Train loss 375 1.187624 Grad Norm 1.696937 2.17s/it\n","Train loss 376 1.833991 Grad Norm 4.238781 1.32s/it\n","Train loss 377 1.818075 Grad Norm 3.002450 1.35s/it\n","Epoch: 126\n","Train loss 378 1.652196 Grad Norm 2.876240 1.62s/it\n","Train loss 379 1.244646 Grad Norm 2.139799 1.93s/it\n","Train loss 380 1.760699 Grad Norm 3.200688 1.31s/it\n","Epoch: 127\n","Train loss 381 1.664287 Grad Norm 2.263674 1.42s/it\n","Train loss 382 1.252021 Grad Norm 1.851765 2.01s/it\n","Train loss 383 1.816741 Grad Norm 3.214024 1.30s/it\n","Epoch: 128\n","Train loss 384 1.753591 Grad Norm 1.814801 1.39s/it\n","Train loss 385 1.736624 Grad Norm 1.743470 1.36s/it\n","Train loss 386 1.211018 Grad Norm 1.881088 1.94s/it\n","Epoch: 129\n","Train loss 387 1.580653 Grad Norm 2.488621 1.71s/it\n","Train loss 388 1.832079 Grad Norm 3.340052 1.32s/it\n","Train loss 389 1.202416 Grad Norm 1.596899 1.94s/it\n","Epoch: 130\n","Train loss 390 1.752026 Grad Norm 2.416661 1.49s/it\n","Train loss 391 1.703776 Grad Norm 2.622874 1.33s/it\n","Train loss 392 1.159128 Grad Norm 1.980266 2.04s/it\n","Epoch: 131\n","Train loss 393 1.608208 Grad Norm 2.918685 1.49s/it\n","Train loss 394 1.721952 Grad Norm 2.536152 1.31s/it\n","Train loss 395 1.215351 Grad Norm 2.413289 1.97s/it\n","Epoch: 132\n","Train loss 396 1.566380 Grad Norm 1.033537 1.44s/it\n","Train loss 397 1.147141 Grad Norm 3.129376 1.96s/it\n","Train loss 398 1.726220 Grad Norm 2.838218 1.41s/it\n","Epoch: 133\n","Train loss 399 1.572324 Grad Norm 3.022058 1.38s/it\n","Train loss 400 1.273392 Grad Norm 1.187554 1.95s/it\n","Train loss 401 1.641148 Grad Norm 1.615169 1.36s/it\n","Epoch: 134\n","Train loss 402 1.672950 Grad Norm 1.294495 1.36s/it\n","Train loss 403 1.164541 Grad Norm 1.715294 1.98s/it\n","Train loss 404 1.837970 Grad Norm 3.602574 1.28s/it\n","Epoch: 135\n","Train loss 405 1.553254 Grad Norm 1.355956 1.52s/it\n","Train loss 406 1.524945 Grad Norm 3.218232 1.36s/it\n","Train loss 407 1.265554 Grad Norm 1.240721 1.99s/it\n","Epoch: 136\n","Train loss 408 1.148593 Grad Norm 1.420815 2.12s/it\n","Train loss 409 1.669647 Grad Norm 2.379723 1.36s/it\n","Train loss 410 1.775389 Grad Norm 6.311757 1.33s/it\n","Epoch: 137\n","Train loss 411 1.277758 Grad Norm 4.272643 2.21s/it\n","Train loss 412 1.571604 Grad Norm 1.486606 1.35s/it\n","Train loss 413 1.688305 Grad Norm 4.704925 1.37s/it\n","Epoch: 138\n","Train loss 414 1.844662 Grad Norm 3.182524 1.43s/it\n","Train loss 415 1.177339 Grad Norm 3.182562 1.94s/it\n","Train loss 416 1.621212 Grad Norm 5.139159 1.35s/it\n","Epoch: 139\n","Train loss 417 1.733159 Grad Norm 1.751525 1.48s/it\n","Train loss 418 1.194606 Grad Norm 3.487659 1.98s/it\n","Train loss 419 1.697761 Grad Norm 5.702173 1.32s/it\n","Epoch: 140\n","Train loss 420 1.724045 Grad Norm 2.421028 1.46s/it\n","Train loss 421 1.235644 Grad Norm 4.887795 1.94s/it\n","Train loss 422 1.708818 Grad Norm 5.725757 1.37s/it\n","Epoch: 141\n","Train loss 423 1.650247 Grad Norm 2.347841 1.48s/it\n","Train loss 424 1.580262 Grad Norm 3.263943 1.30s/it\n","Train loss 425 1.248376 Grad Norm 3.021996 1.96s/it\n","Epoch: 142\n","Train loss 426 1.234493 Grad Norm 1.820528 2.23s/it\n","Train loss 427 1.615228 Grad Norm 3.325372 1.34s/it\n","Train loss 428 1.636959 Grad Norm 2.762298 1.34s/it\n","Epoch: 143\n","Train loss 429 1.151044 Grad Norm 1.806783 2.37s/it\n","Train loss 430 1.642006 Grad Norm 1.969198 1.28s/it\n","Train loss 431 1.555493 Grad Norm 2.183742 1.37s/it\n","Epoch: 144\n","Train loss 432 1.587649 Grad Norm 2.165045 1.56s/it\n","Train loss 433 1.137408 Grad Norm 1.207968 1.97s/it\n","Train loss 434 1.607345 Grad Norm 2.242121 1.29s/it\n","Epoch: 145\n","Train loss 435 1.585090 Grad Norm 1.980091 1.50s/it\n","Train loss 436 1.121590 Grad Norm 2.406052 1.94s/it\n","Train loss 437 1.588678 Grad Norm 1.570992 1.29s/it\n","Epoch: 146\n","Train loss 438 1.218076 Grad Norm 1.829060 2.14s/it\n","Train loss 439 1.530531 Grad Norm 1.927104 1.38s/it\n","Train loss 440 1.572188 Grad Norm 1.930371 1.30s/it\n","Epoch: 147\n","Train loss 441 1.100614 Grad Norm 1.091464 2.17s/it\n","Train loss 442 1.588388 Grad Norm 2.350119 1.31s/it\n","Train loss 443 1.571013 Grad Norm 2.358567 1.34s/it\n","Epoch: 148\n","Train loss 444 1.614865 Grad Norm 3.097141 1.49s/it\n","Train loss 445 1.036510 Grad Norm 1.413853 1.95s/it\n","Train loss 446 1.687029 Grad Norm 4.078521 1.29s/it\n","Epoch: 149\n","Train loss 447 1.608239 Grad Norm 2.124218 1.49s/it\n","Train loss 448 1.514429 Grad Norm 3.798638 1.35s/it\n","Train loss 449 1.532829 Grad Norm 3.164931 1.33s/it\n","Epoch: 150\n","Train loss 450 1.512124 Grad Norm 1.461004 1.45s/it\n","Train loss 451 1.576924 Grad Norm 2.471382 1.31s/it\n","Train loss 452 1.138961 Grad Norm 2.189106 1.99s/it\n","Epoch: 151\n","Train loss 453 1.656974 Grad Norm 3.537802 1.54s/it\n","Train loss 454 1.028794 Grad Norm 0.698358 1.97s/it\n","Train loss 455 1.494627 Grad Norm 3.347569 1.37s/it\n","Epoch: 152\n","Train loss 456 1.059247 Grad Norm 1.431676 2.09s/it\n","Train loss 457 1.543433 Grad Norm 3.873437 1.33s/it\n","Train loss 458 1.562453 Grad Norm 3.890632 1.35s/it\n","Epoch: 153\n","Train loss 459 1.504930 Grad Norm 1.721931 1.57s/it\n","Train loss 460 1.510689 Grad Norm 2.342831 1.33s/it\n","Train loss 461 1.049726 Grad Norm 2.744147 1.98s/it\n","Epoch: 154\n","Train loss 462 1.547685 Grad Norm 2.745515 1.50s/it\n","Train loss 463 1.627882 Grad Norm 3.440650 1.29s/it\n","Train loss 464 0.952803 Grad Norm 0.829856 1.95s/it\n","Epoch: 155\n","Train loss 465 1.397274 Grad Norm 1.076850 1.46s/it\n","Train loss 466 1.083217 Grad Norm 1.176842 1.93s/it\n","Train loss 467 1.570323 Grad Norm 1.203687 1.36s/it\n","Epoch: 156\n","Train loss 468 1.425568 Grad Norm 2.062640 1.47s/it\n","Train loss 469 1.674962 Grad Norm 2.546099 1.28s/it\n","Train loss 470 1.062788 Grad Norm 1.072787 1.96s/it\n","Epoch: 157\n","Train loss 471 1.439822 Grad Norm 1.991822 1.48s/it\n","Train loss 472 1.117298 Grad Norm 1.512353 2.19s/it\n","Train loss 473 1.406695 Grad Norm 1.147324 1.32s/it\n","Epoch: 158\n","Train loss 474 1.573098 Grad Norm 2.777866 1.51s/it\n","Train loss 475 1.484894 Grad Norm 1.332485 1.35s/it\n","Train loss 476 1.009700 Grad Norm 1.069299 1.97s/it\n","Epoch: 159\n","Train loss 477 1.459909 Grad Norm 1.986952 1.44s/it\n","Train loss 478 1.049687 Grad Norm 2.820275 1.96s/it\n","Train loss 479 1.664540 Grad Norm 2.952265 1.33s/it\n","Epoch: 160\n","Train loss 480 1.645762 Grad Norm 1.801712 1.36s/it\n","Train loss 481 1.044678 Grad Norm 1.110696 2.01s/it\n","Train loss 482 1.349859 Grad Norm 1.416443 1.36s/it\n","Epoch: 161\n","Train loss 483 1.488281 Grad Norm 1.482934 1.48s/it\n","Train loss 484 1.473412 Grad Norm 1.122663 1.32s/it\n","Train loss 485 0.972501 Grad Norm 1.064664 1.97s/it\n","Epoch: 162\n","Train loss 486 1.467726 Grad Norm 2.348894 1.47s/it\n","Train loss 487 1.018417 Grad Norm 1.338477 1.97s/it\n","Train loss 488 1.508219 Grad Norm 3.328156 1.31s/it\n","Epoch: 163\n","Train loss 489 1.477085 Grad Norm 1.819747 1.43s/it\n","Train loss 490 1.478591 Grad Norm 4.147594 1.30s/it\n","Train loss 491 1.112575 Grad Norm 3.090469 1.99s/it\n","Epoch: 164\n","Train loss 492 1.474543 Grad Norm 2.053878 1.49s/it\n","Train loss 493 1.005012 Grad Norm 0.928691 1.95s/it\n","Train loss 494 1.379712 Grad Norm 1.981429 1.33s/it\n","Epoch: 165\n","Train loss 495 1.502619 Grad Norm 1.553183 1.51s/it\n","Train loss 496 1.024544 Grad Norm 1.170532 1.98s/it\n","Train loss 497 1.399233 Grad Norm 1.332659 1.36s/it\n","Epoch: 166\n","Train loss 498 1.421797 Grad Norm 1.065912 1.39s/it\n","Train loss 499 1.495472 Grad Norm 2.992596 1.31s/it\n","Train loss 500 1.009811 Grad Norm 1.345376 1.93s/it\n","Epoch: 167\n","Train loss 501 1.557776 Grad Norm 5.279641 1.43s/it\n","Train loss 502 1.136478 Grad Norm 3.459202 2.00s/it\n","Train loss 503 1.281409 Grad Norm 1.642139 1.36s/it\n","Epoch: 168\n","Train loss 504 0.999730 Grad Norm 2.289801 2.13s/it\n","Train loss 505 1.333837 Grad Norm 1.952430 1.36s/it\n","Train loss 506 1.597756 Grad Norm 4.519513 1.31s/it\n","Epoch: 169\n","Train loss 507 1.003017 Grad Norm 1.870179 2.12s/it\n","Train loss 508 1.534040 Grad Norm 3.914062 1.28s/it\n","Train loss 509 1.461976 Grad Norm 3.733365 1.34s/it\n","Epoch: 170\n","Train loss 510 1.470190 Grad Norm 2.246034 1.48s/it\n","Train loss 511 1.390137 Grad Norm 1.611156 1.62s/it\n","Train loss 512 0.956793 Grad Norm 2.955331 1.95s/it\n","Epoch: 171\n","Train loss 513 0.949825 Grad Norm 1.603050 2.17s/it\n","Train loss 514 1.410775 Grad Norm 4.262624 1.38s/it\n","Train loss 515 1.460230 Grad Norm 4.221668 1.35s/it\n","Epoch: 172\n","Train loss 516 0.933874 Grad Norm 0.690077 2.20s/it\n","Train loss 517 1.463728 Grad Norm 3.214413 1.29s/it\n","Train loss 518 1.432868 Grad Norm 1.463173 1.37s/it\n","Epoch: 173\n","Train loss 519 1.405898 Grad Norm 3.402555 1.55s/it\n","Train loss 520 0.993857 Grad Norm 1.566425 2.00s/it\n","Train loss 521 1.419352 Grad Norm 3.554908 1.32s/it\n","Epoch: 174\n","Train loss 522 1.459220 Grad Norm 3.354099 1.54s/it\n","Train loss 523 1.353434 Grad Norm 2.131707 1.32s/it\n","Train loss 524 1.013881 Grad Norm 1.115519 1.96s/it\n","Epoch: 175\n","Train loss 525 1.348034 Grad Norm 3.365404 1.49s/it\n","Train loss 526 1.003320 Grad Norm 2.413024 2.00s/it\n","Train loss 527 1.365301 Grad Norm 2.015756 1.33s/it\n","Epoch: 176\n","Train loss 528 1.382156 Grad Norm 2.242166 1.49s/it\n","Train loss 529 1.460427 Grad Norm 4.360013 1.33s/it\n","Train loss 530 0.899620 Grad Norm 1.516362 1.99s/it\n","Epoch: 177\n","Train loss 531 1.431882 Grad Norm 2.714900 1.58s/it\n","Train loss 532 1.258495 Grad Norm 1.994229 1.30s/it\n","Train loss 533 0.966950 Grad Norm 1.811853 1.95s/it\n","Epoch: 178\n","Train loss 534 1.374375 Grad Norm 1.330104 1.46s/it\n","Train loss 535 1.033940 Grad Norm 1.757523 1.98s/it\n","Train loss 536 1.222521 Grad Norm 1.327484 1.38s/it\n","Epoch: 179\n","Train loss 537 0.914495 Grad Norm 0.880274 2.13s/it\n","Train loss 538 1.491122 Grad Norm 2.124055 1.24s/it\n","Train loss 539 1.391054 Grad Norm 1.636364 1.33s/it\n","Epoch: 180\n","Train loss 540 1.375059 Grad Norm 1.745943 1.46s/it\n","Train loss 541 1.425791 Grad Norm 2.196355 1.25s/it\n","Train loss 542 0.931852 Grad Norm 0.869388 1.96s/it\n","Epoch: 181\n","Train loss 543 1.365916 Grad Norm 1.503550 1.61s/it\n","Train loss 544 1.000361 Grad Norm 1.218470 1.99s/it\n","Train loss 545 1.238335 Grad Norm 1.997670 1.33s/it\n","Epoch: 182\n","Train loss 546 1.282018 Grad Norm 1.920668 1.41s/it\n","Train loss 547 1.558484 Grad Norm 1.742898 1.26s/it\n","Train loss 548 0.971721 Grad Norm 2.953449 2.23s/it\n","Epoch: 183\n","Train loss 549 1.254373 Grad Norm 1.500958 1.55s/it\n","Train loss 550 0.994196 Grad Norm 2.012584 2.02s/it\n","Train loss 551 1.336074 Grad Norm 1.634993 1.30s/it\n","Epoch: 184\n","Train loss 552 1.283964 Grad Norm 1.180989 1.47s/it\n","Train loss 553 1.286835 Grad Norm 1.734629 1.31s/it\n","Train loss 554 0.983707 Grad Norm 1.459374 1.97s/it\n","Epoch: 185\n","Train loss 555 1.413203 Grad Norm 1.355293 1.45s/it\n","Train loss 556 1.300058 Grad Norm 1.491543 1.36s/it\n","Train loss 557 1.273657 Grad Norm 2.456850 1.36s/it\n","Epoch: 186\n","Train loss 558 1.271673 Grad Norm 3.196259 1.52s/it\n","Train loss 559 1.380664 Grad Norm 1.579614 1.29s/it\n","Train loss 560 0.981578 Grad Norm 3.260188 1.96s/it\n","Epoch: 187\n","Train loss 561 0.951923 Grad Norm 3.255473 2.11s/it\n","Train loss 562 1.536490 Grad Norm 4.110225 1.38s/it\n","Train loss 563 1.374313 Grad Norm 2.673934 1.20s/it\n","Epoch: 188\n","Train loss 564 1.276135 Grad Norm 2.175273 1.56s/it\n","Train loss 565 1.366061 Grad Norm 1.867785 1.33s/it\n","Train loss 566 1.387278 Grad Norm 2.732343 1.28s/it\n","Epoch: 189\n","Train loss 567 0.958049 Grad Norm 2.077034 2.10s/it\n","Train loss 568 1.259312 Grad Norm 1.771069 1.28s/it\n","Train loss 569 1.309600 Grad Norm 1.687374 1.37s/it\n","Epoch: 190\n","Train loss 570 0.912274 Grad Norm 0.982177 2.13s/it\n","Train loss 571 1.355529 Grad Norm 2.387897 1.40s/it\n","Train loss 572 1.341568 Grad Norm 3.187955 1.23s/it\n","Epoch: 191\n","Train loss 573 1.312157 Grad Norm 1.551406 1.44s/it\n","Train loss 574 1.299185 Grad Norm 1.743781 1.33s/it\n","Train loss 575 0.848692 Grad Norm 1.018204 1.99s/it\n","Epoch: 192\n","Train loss 576 0.936050 Grad Norm 1.276072 2.13s/it\n","Train loss 577 1.214581 Grad Norm 1.237617 1.37s/it\n","Train loss 578 1.252183 Grad Norm 1.231569 1.39s/it\n","Epoch: 193\n","Train loss 579 0.961743 Grad Norm 2.649300 2.13s/it\n","Train loss 580 1.246107 Grad Norm 1.937530 1.34s/it\n","Train loss 581 1.205221 Grad Norm 1.281475 1.36s/it\n","Epoch: 194\n","Train loss 582 1.263736 Grad Norm 1.652443 1.45s/it\n","Train loss 583 0.931853 Grad Norm 1.776892 2.00s/it\n","Train loss 584 1.262499 Grad Norm 1.879708 1.33s/it\n","Epoch: 195\n","Train loss 585 1.310434 Grad Norm 1.756549 1.54s/it\n","Train loss 586 0.879731 Grad Norm 9.884112 1.99s/it\n","Train loss 587 1.397855 Grad Norm 3.657848 1.33s/it\n","Epoch: 196\n","Train loss 588 1.384384 Grad Norm 4.077448 1.45s/it\n","Train loss 589 0.859718 Grad Norm 0.984307 1.98s/it\n","Train loss 590 1.335446 Grad Norm 2.136945 1.53s/it\n","Epoch: 197\n","Train loss 591 0.899832 Grad Norm 0.905217 2.19s/it\n","Train loss 592 1.196572 Grad Norm 1.663590 1.40s/it\n","Train loss 593 1.327508 Grad Norm 5.676091 1.34s/it\n","Epoch: 198\n","Train loss 594 1.271283 Grad Norm 4.364710 1.47s/it\n","Train loss 595 0.941359 Grad Norm 1.233191 1.97s/it\n","Train loss 596 1.230527 Grad Norm 1.726246 1.38s/it\n","Epoch: 199\n","Train loss 597 1.215427 Grad Norm 1.755046 1.48s/it\n","Train loss 598 0.897906 Grad Norm 1.976067 2.04s/it\n","Train loss 599 1.332602 Grad Norm 3.163080 1.34s/it\n","Epoch: 200\n","Train loss 600 1.399493 Grad Norm 3.266890 1.38s/it\n","Train loss 601 1.281480 Grad Norm 2.919838 1.30s/it\n","Train loss 602 1.197263 Grad Norm 3.865113 1.36s/it\n","Epoch: 201\n","Train loss 603 1.257936 Grad Norm 3.159693 1.45s/it\n","Train loss 604 0.995369 Grad Norm 3.851474 1.98s/it\n","Train loss 605 1.314060 Grad Norm 5.303542 1.38s/it\n","Epoch: 202\n","Train loss 606 1.281792 Grad Norm 1.115116 1.56s/it\n","Train loss 607 0.862301 Grad Norm 1.556300 1.94s/it\n","Train loss 608 1.250143 Grad Norm 2.468438 1.33s/it\n","Epoch: 203\n","Train loss 609 1.350110 Grad Norm 5.662707 1.46s/it\n","Train loss 610 1.261090 Grad Norm 4.133482 1.32s/it\n","Train loss 611 0.920120 Grad Norm 1.722290 1.97s/it\n","Epoch: 204\n","Train loss 612 0.942114 Grad Norm 3.786867 2.19s/it\n","Train loss 613 1.571612 Grad Norm 7.472913 1.35s/it\n","Train loss 614 1.344143 Grad Norm 4.706741 1.31s/it\n","Epoch: 205\n","Train loss 615 0.889420 Grad Norm 2.023393 2.19s/it\n","Train loss 616 1.342039 Grad Norm 2.269121 1.32s/it\n","Train loss 617 1.292486 Grad Norm 2.109353 1.33s/it\n","Epoch: 206\n","Train loss 618 0.844395 Grad Norm 1.605219 2.18s/it\n","Train loss 619 1.378952 Grad Norm 2.270712 1.34s/it\n","Train loss 620 1.190757 Grad Norm 4.065766 1.41s/it\n","Epoch: 207\n","Train loss 621 1.283877 Grad Norm 1.646375 1.37s/it\n","Train loss 622 0.877671 Grad Norm 1.089173 1.96s/it\n","Train loss 623 1.338857 Grad Norm 2.393914 1.33s/it\n","Epoch: 208\n","Train loss 624 1.325158 Grad Norm 3.559598 1.42s/it\n","Train loss 625 1.203626 Grad Norm 3.696981 1.36s/it\n","Train loss 626 0.867398 Grad Norm 0.819225 2.23s/it\n","Epoch: 209\n","Train loss 627 1.227880 Grad Norm 2.320298 1.45s/it\n","Train loss 628 0.847325 Grad Norm 0.985547 1.97s/it\n","Train loss 629 1.231606 Grad Norm 2.845586 1.40s/it\n","Epoch: 210\n","Train loss 630 1.207525 Grad Norm 2.625625 1.47s/it\n","Train loss 631 0.847579 Grad Norm 0.917097 1.95s/it\n","Train loss 632 1.260517 Grad Norm 2.730281 1.34s/it\n","Epoch: 211\n","Train loss 633 1.130151 Grad Norm 1.809671 1.56s/it\n","Train loss 634 0.837954 Grad Norm 1.014103 2.00s/it\n","Train loss 635 1.288517 Grad Norm 2.782922 1.32s/it\n","Epoch: 212\n","Train loss 636 1.141197 Grad Norm 1.394390 1.48s/it\n","Train loss 637 0.847006 Grad Norm 2.188286 1.96s/it\n","Train loss 638 1.373046 Grad Norm 2.114616 1.24s/it\n","Epoch: 213\n","Train loss 639 1.104396 Grad Norm 0.960111 1.40s/it\n","Train loss 640 1.336525 Grad Norm 3.274970 1.33s/it\n","Train loss 641 1.181332 Grad Norm 1.855210 1.40s/it\n","Epoch: 214\n","Train loss 642 1.345166 Grad Norm 2.874105 1.37s/it\n","Train loss 643 1.165148 Grad Norm 1.534407 1.35s/it\n","Train loss 644 1.195882 Grad Norm 2.192246 1.30s/it\n","Epoch: 215\n","Train loss 645 1.224712 Grad Norm 1.470450 1.43s/it\n","Train loss 646 0.861040 Grad Norm 1.653934 1.99s/it\n","Train loss 647 1.153868 Grad Norm 1.820564 1.33s/it\n","Epoch: 216\n","Train loss 648 0.930262 Grad Norm 1.818320 2.23s/it\n","Train loss 649 1.161443 Grad Norm 1.811876 1.39s/it\n","Train loss 650 1.188525 Grad Norm 1.244504 1.24s/it\n","Epoch: 217\n","Train loss 651 1.246128 Grad Norm 1.395032 1.45s/it\n","Train loss 652 0.822223 Grad Norm 1.263634 1.94s/it\n","Train loss 653 1.194494 Grad Norm 3.590292 1.34s/it\n","Epoch: 218\n","Train loss 654 1.136254 Grad Norm 2.219995 1.56s/it\n","Train loss 655 0.870414 Grad Norm 3.147105 1.98s/it\n","Train loss 656 1.395315 Grad Norm 5.581288 1.25s/it\n","Epoch: 219\n","Train loss 657 1.215084 Grad Norm 2.103580 1.43s/it\n","Train loss 658 1.348810 Grad Norm 7.163468 1.36s/it\n","Train loss 659 0.918410 Grad Norm 4.395804 1.97s/it\n","Epoch: 220\n","Train loss 660 0.916484 Grad Norm 3.383921 2.13s/it\n","Train loss 661 1.226610 Grad Norm 1.796634 1.28s/it\n","Train loss 662 1.274015 Grad Norm 3.482095 1.32s/it\n","Epoch: 221\n","Train loss 663 1.178002 Grad Norm 1.849852 1.48s/it\n","Train loss 664 0.849900 Grad Norm 1.764282 2.00s/it\n","Train loss 665 1.264085 Grad Norm 3.499252 1.31s/it\n","Epoch: 222\n","Train loss 666 1.218084 Grad Norm 3.004796 1.43s/it\n","Train loss 667 1.127629 Grad Norm 2.072200 1.40s/it\n","Train loss 668 0.825387 Grad Norm 1.324413 1.98s/it\n","Epoch: 223\n","Train loss 669 1.288097 Grad Norm 3.924295 1.71s/it\n","Train loss 670 1.180061 Grad Norm 3.264904 1.37s/it\n","Train loss 671 0.816994 Grad Norm 1.941637 1.96s/it\n","Epoch: 224\n","Train loss 672 1.310530 Grad Norm 3.990077 1.37s/it\n","Train loss 673 0.806743 Grad Norm 1.322452 1.98s/it\n","Train loss 674 1.135410 Grad Norm 2.279222 1.31s/it\n","Epoch: 225\n","Train loss 675 1.114598 Grad Norm 1.991707 1.48s/it\n","Train loss 676 1.162584 Grad Norm 2.679279 1.41s/it\n","Train loss 677 0.828297 Grad Norm 1.718317 2.00s/it\n","Epoch: 226\n","Train loss 678 0.851813 Grad Norm 1.401513 2.14s/it\n","Train loss 679 1.063035 Grad Norm 1.019167 1.32s/it\n","Train loss 680 1.119129 Grad Norm 2.553259 1.37s/it\n","Epoch: 227\n","Train loss 681 0.844084 Grad Norm 1.388432 2.17s/it\n","Train loss 682 1.049585 Grad Norm 1.248283 1.34s/it\n","Train loss 683 1.197166 Grad Norm 2.006402 1.35s/it\n","Epoch: 228\n","Train loss 684 1.225654 Grad Norm 3.576274 1.50s/it\n","Train loss 685 0.825985 Grad Norm 1.341618 1.96s/it\n","Train loss 686 1.273531 Grad Norm 3.334876 1.16s/it\n","Epoch: 229\n","Train loss 687 1.173222 Grad Norm 1.612010 1.45s/it\n","Train loss 688 0.887153 Grad Norm 1.260440 1.98s/it\n","Train loss 689 1.076184 Grad Norm 1.586197 1.37s/it\n","Epoch: 230\n","Train loss 690 1.103080 Grad Norm 2.028655 1.50s/it\n","Train loss 691 0.778939 Grad Norm 1.570868 2.00s/it\n","Train loss 692 1.221525 Grad Norm 2.192584 1.32s/it\n","Epoch: 231\n","Train loss 693 0.772311 Grad Norm 1.282740 2.12s/it\n","Train loss 694 1.189627 Grad Norm 2.698530 1.31s/it\n","Train loss 695 1.251129 Grad Norm 1.992984 1.33s/it\n","Epoch: 232\n","Train loss 696 0.817231 Grad Norm 1.603265 2.31s/it\n","Train loss 697 1.094203 Grad Norm 1.523206 1.37s/it\n","Train loss 698 1.162990 Grad Norm 2.143943 1.30s/it\n","Epoch: 233\n","Train loss 699 1.094630 Grad Norm 2.210809 1.47s/it\n","Train loss 700 0.788014 Grad Norm 1.959865 1.95s/it\n","Train loss 701 1.161081 Grad Norm 2.556751 1.33s/it\n","Epoch: 234\n","Train loss 702 1.213063 Grad Norm 1.341196 1.40s/it\n","Train loss 703 1.101862 Grad Norm 1.121556 1.31s/it\n","Train loss 704 1.039656 Grad Norm 1.585650 1.34s/it\n","Epoch: 235\n","Train loss 705 0.843415 Grad Norm 1.081212 2.09s/it\n","Train loss 706 1.174870 Grad Norm 2.361066 1.32s/it\n","Train loss 707 1.121516 Grad Norm 2.004108 1.27s/it\n","Epoch: 236\n","Train loss 708 1.173046 Grad Norm 1.854089 1.45s/it\n","Train loss 709 1.107035 Grad Norm 2.720818 1.36s/it\n","Train loss 710 0.798813 Grad Norm 1.136925 2.00s/it\n","Epoch: 237\n","Train loss 711 1.097559 Grad Norm 3.295923 1.52s/it\n","Train loss 712 1.182700 Grad Norm 5.693592 1.33s/it\n","Train loss 713 0.898613 Grad Norm 2.652196 2.22s/it\n","Epoch: 238\n","Train loss 714 0.802350 Grad Norm 1.306761 2.15s/it\n","Train loss 715 1.161060 Grad Norm 2.454637 1.40s/it\n","Train loss 716 1.247286 Grad Norm 3.212367 1.34s/it\n","Epoch: 239\n","Train loss 717 1.263721 Grad Norm 2.045031 1.58s/it\n","Train loss 718 0.828306 Grad Norm 1.490731 1.98s/it\n","Train loss 719 1.029761 Grad Norm 2.100609 1.33s/it\n","Epoch: 240\n","Train loss 720 1.202944 Grad Norm 2.764049 1.49s/it\n","Train loss 721 1.102996 Grad Norm 2.051351 1.28s/it\n","Train loss 722 0.775767 Grad Norm 0.973634 1.98s/it\n","Epoch: 241\n","Train loss 723 1.095645 Grad Norm 1.595899 1.51s/it\n","Train loss 724 0.747663 Grad Norm 0.889049 2.00s/it\n","Train loss 725 1.118298 Grad Norm 1.306953 1.32s/it\n","Epoch: 242\n","Train loss 726 0.791466 Grad Norm 0.917201 2.11s/it\n","Train loss 727 1.103108 Grad Norm 1.742835 1.32s/it\n","Train loss 728 1.135042 Grad Norm 1.254707 1.31s/it\n","Epoch: 243\n","Train loss 729 1.148633 Grad Norm 2.421442 1.47s/it\n","Train loss 730 0.764275 Grad Norm 1.765870 2.03s/it\n","Train loss 731 1.091506 Grad Norm 1.280823 1.37s/it\n","Epoch: 244\n","Train loss 732 0.779507 Grad Norm 1.333682 2.16s/it\n","Train loss 733 1.057306 Grad Norm 1.387104 1.35s/it\n","Train loss 734 1.120458 Grad Norm 2.399415 1.33s/it\n","Epoch: 245\n","Train loss 735 1.088147 Grad Norm 1.934495 1.45s/it\n","Train loss 736 1.037328 Grad Norm 1.561243 1.35s/it\n","Train loss 737 1.181582 Grad Norm 2.011769 1.33s/it\n","Epoch: 246\n","Train loss 738 0.834219 Grad Norm 0.964497 2.26s/it\n","Train loss 739 1.040394 Grad Norm 2.579363 1.37s/it\n","Train loss 740 1.132596 Grad Norm 2.191310 1.23s/it\n","Epoch: 247\n","Train loss 741 0.788431 Grad Norm 1.568397 2.11s/it\n","Train loss 742 1.135946 Grad Norm 2.290579 1.32s/it\n","Train loss 743 1.072464 Grad Norm 2.183920 1.36s/it\n","Epoch: 248\n","Train loss 744 0.730941 Grad Norm 1.229259 2.18s/it\n","Train loss 745 1.131081 Grad Norm 2.322648 1.32s/it\n","Train loss 746 1.177125 Grad Norm 1.965602 1.24s/it\n","Epoch: 249\n","Train loss 747 1.073724 Grad Norm 2.437184 1.48s/it\n","Train loss 748 1.088890 Grad Norm 2.296633 1.32s/it\n","Train loss 749 0.768280 Grad Norm 1.647444 2.21s/it\n","Epoch: 250\n","Train loss 750 1.055011 Grad Norm 2.256818 1.42s/it\n","Train loss 751 0.771160 Grad Norm 1.049494 2.02s/it\n","Train loss 752 1.019187 Grad Norm 1.460612 1.38s/it\n","Epoch: 251\n","Train loss 753 1.121243 Grad Norm 1.212120 1.43s/it\n","Train loss 754 1.079757 Grad Norm 1.672077 1.29s/it\n","Train loss 755 0.716876 Grad Norm 0.930202 1.94s/it\n","Epoch: 252\n","Train loss 756 1.094561 Grad Norm 1.864186 1.51s/it\n","Train loss 757 1.103229 Grad Norm 1.478096 1.16s/it\n","Train loss 758 1.044988 Grad Norm 1.914943 1.39s/it\n","Epoch: 253\n","Train loss 759 1.071805 Grad Norm 1.776426 1.47s/it\n","Train loss 760 0.740812 Grad Norm 1.021923 2.00s/it\n","Train loss 761 1.077925 Grad Norm 1.972470 1.36s/it\n","Epoch: 254\n","Train loss 762 1.019958 Grad Norm 1.389838 1.45s/it\n","Train loss 763 0.748779 Grad Norm 1.166602 1.97s/it\n","Train loss 764 1.129915 Grad Norm 1.996840 1.31s/it\n","Epoch: 255\n","Train loss 765 0.760410 Grad Norm 1.075054 2.16s/it\n","Train loss 766 1.134796 Grad Norm 2.142925 1.20s/it\n","Train loss 767 1.048976 Grad Norm 1.781683 1.37s/it\n","Epoch: 256\n","Train loss 768 1.121842 Grad Norm 3.570360 1.39s/it\n","Train loss 769 1.102053 Grad Norm 2.287507 1.32s/it\n","Train loss 770 0.739794 Grad Norm 1.526809 1.96s/it\n","Epoch: 257\n","Train loss 771 0.963274 Grad Norm 2.022419 1.48s/it\n","Train loss 772 0.799823 Grad Norm 1.496703 1.96s/it\n","Train loss 773 1.051185 Grad Norm 2.363747 1.31s/it\n","Epoch: 258\n","Train loss 774 1.040865 Grad Norm 1.744662 1.46s/it\n","Train loss 775 1.106641 Grad Norm 1.721704 1.21s/it\n","Train loss 776 0.727457 Grad Norm 1.178543 1.97s/it\n","Epoch: 259\n","Train loss 777 0.992269 Grad Norm 1.917749 1.50s/it\n","Train loss 778 0.769209 Grad Norm 2.243693 1.98s/it\n","Train loss 779 1.045823 Grad Norm 2.144256 1.27s/it\n","Epoch: 260\n","Train loss 780 1.003289 Grad Norm 1.447938 1.57s/it\n","Train loss 781 1.018662 Grad Norm 1.803804 1.32s/it\n","Train loss 782 1.073035 Grad Norm 1.483745 1.28s/it\n","Epoch: 261\n","Train loss 783 1.015352 Grad Norm 1.721576 1.49s/it\n","Train loss 784 1.020852 Grad Norm 1.544714 1.29s/it\n","Train loss 785 0.711014 Grad Norm 0.812752 1.97s/it\n","Epoch: 262\n","Train loss 786 1.047583 Grad Norm 1.330890 1.49s/it\n","Train loss 787 1.096778 Grad Norm 1.879934 1.18s/it\n","Train loss 788 0.709627 Grad Norm 0.970070 1.99s/it\n","Epoch: 263\n","Train loss 789 0.710517 Grad Norm 1.787777 2.17s/it\n","Train loss 790 1.112528 Grad Norm 3.211337 1.35s/it\n","Train loss 791 0.994755 Grad Norm 2.050347 1.56s/it\n","Epoch: 264\n","Train loss 792 1.019449 Grad Norm 1.868600 1.47s/it\n","Train loss 793 0.759612 Grad Norm 1.835314 1.98s/it\n","Train loss 794 0.984340 Grad Norm 1.671671 1.34s/it\n","Epoch: 265\n","Train loss 795 0.993093 Grad Norm 1.926820 1.46s/it\n","Train loss 796 0.994865 Grad Norm 1.119021 1.37s/it\n","Train loss 797 0.730113 Grad Norm 1.285111 1.92s/it\n","Epoch: 266\n","Train loss 798 1.098894 Grad Norm 2.176226 1.53s/it\n","Train loss 799 0.767700 Grad Norm 2.310069 1.97s/it\n","Train loss 800 1.024526 Grad Norm 2.833844 1.20s/it\n","Epoch: 267\n","Train loss 801 0.733061 Grad Norm 0.992068 2.19s/it\n","Train loss 802 0.994122 Grad Norm 2.979418 1.32s/it\n","Train loss 803 1.178464 Grad Norm 4.569144 1.25s/it\n","Epoch: 268\n","Train loss 804 1.073427 Grad Norm 2.950803 1.49s/it\n","Train loss 805 1.016998 Grad Norm 2.431833 1.32s/it\n","Train loss 806 0.686299 Grad Norm 0.992020 1.94s/it\n","Epoch: 269\n","Train loss 807 0.827411 Grad Norm 2.323681 2.33s/it\n","Train loss 808 0.994494 Grad Norm 3.004349 1.33s/it\n","Train loss 809 0.991233 Grad Norm 1.369582 1.29s/it\n","Epoch: 270\n","Train loss 810 1.052497 Grad Norm 1.376465 1.43s/it\n","Train loss 811 1.008192 Grad Norm 1.537989 1.37s/it\n","Train loss 812 0.644039 Grad Norm 0.801588 2.00s/it\n","Epoch: 271\n","Train loss 813 0.748903 Grad Norm 0.988510 2.14s/it\n","Train loss 814 0.956957 Grad Norm 1.222981 1.32s/it\n","Train loss 815 0.959527 Grad Norm 2.096176 1.37s/it\n","Epoch: 272\n","Train loss 816 1.001799 Grad Norm 1.724191 1.47s/it\n","Train loss 817 0.978793 Grad Norm 2.381508 1.39s/it\n","Train loss 818 0.685200 Grad Norm 1.266637 1.98s/it\n","Epoch: 273\n","Train loss 819 0.768805 Grad Norm 2.321934 2.13s/it\n","Train loss 820 0.997051 Grad Norm 2.441232 1.26s/it\n","Train loss 821 1.049127 Grad Norm 1.801423 1.28s/it\n","Epoch: 274\n","Train loss 822 1.007498 Grad Norm 3.119294 1.47s/it\n","Train loss 823 1.122587 Grad Norm 3.430883 1.28s/it\n","Train loss 824 0.686907 Grad Norm 1.236628 1.99s/it\n","Epoch: 275\n","Train loss 825 1.061666 Grad Norm 2.796733 1.46s/it\n","Train loss 826 0.706844 Grad Norm 1.829385 1.99s/it\n","Train loss 827 0.980033 Grad Norm 2.268429 1.34s/it\n","Epoch: 276\n","Train loss 828 0.981746 Grad Norm 1.540489 1.53s/it\n","Train loss 829 0.961742 Grad Norm 1.354646 1.31s/it\n","Train loss 830 0.673818 Grad Norm 0.948673 1.99s/it\n","Epoch: 277\n","Train loss 831 0.985026 Grad Norm 2.349885 1.45s/it\n","Train loss 832 1.031284 Grad Norm 1.581650 1.30s/it\n","Train loss 833 0.701466 Grad Norm 1.472115 1.99s/it\n","Epoch: 278\n","Train loss 834 0.934183 Grad Norm 1.282703 1.49s/it\n","Train loss 835 0.727428 Grad Norm 1.070567 2.02s/it\n","Train loss 836 0.983336 Grad Norm 2.328656 1.29s/it\n","Epoch: 279\n","Train loss 837 0.670288 Grad Norm 1.104941 2.12s/it\n","Train loss 838 0.916084 Grad Norm 1.264179 1.55s/it\n","Train loss 839 1.003670 Grad Norm 1.226749 1.34s/it\n","Epoch: 280\n","Train loss 840 0.976661 Grad Norm 1.242284 1.42s/it\n","Train loss 841 0.944996 Grad Norm 1.157583 1.27s/it\n","Train loss 842 0.668216 Grad Norm 0.748231 2.01s/it\n","Epoch: 281\n","Train loss 843 0.711288 Grad Norm 1.132774 2.13s/it\n","Train loss 844 0.947385 Grad Norm 1.372278 1.30s/it\n","Train loss 845 0.922798 Grad Norm 1.415644 1.33s/it\n","Epoch: 282\n","Train loss 846 0.669043 Grad Norm 0.903856 2.16s/it\n","Train loss 847 0.901150 Grad Norm 1.410248 1.40s/it\n","Train loss 848 0.999775 Grad Norm 1.920172 1.32s/it\n","Epoch: 283\n","Train loss 849 0.646856 Grad Norm 0.845511 2.30s/it\n","Train loss 850 1.012224 Grad Norm 2.833216 1.31s/it\n","Train loss 851 0.930492 Grad Norm 1.266236 1.31s/it\n","Epoch: 284\n","Train loss 852 0.902886 Grad Norm 2.169851 1.50s/it\n","Train loss 853 0.965034 Grad Norm 2.006288 1.25s/it\n","Train loss 854 1.082186 Grad Norm 3.290888 1.30s/it\n","Epoch: 285\n","Train loss 855 1.037316 Grad Norm 3.492172 1.50s/it\n","Train loss 856 0.860479 Grad Norm 1.360046 1.33s/it\n","Train loss 857 0.679137 Grad Norm 1.431755 1.96s/it\n","Epoch: 286\n","Train loss 858 0.952700 Grad Norm 1.865604 1.42s/it\n","Train loss 859 0.890507 Grad Norm 1.782405 1.29s/it\n","Train loss 860 0.753774 Grad Norm 1.792004 1.94s/it\n","Epoch: 287\n","Train loss 861 0.924642 Grad Norm 1.372672 1.48s/it\n","Train loss 862 0.976026 Grad Norm 1.902694 1.29s/it\n","Train loss 863 0.638856 Grad Norm 1.335933 2.00s/it\n","Epoch: 288\n","Train loss 864 0.660166 Grad Norm 1.144463 2.11s/it\n","Train loss 865 0.917024 Grad Norm 1.261651 1.28s/it\n","Train loss 866 0.974710 Grad Norm 1.712147 1.24s/it\n","Epoch: 289\n","Train loss 867 0.672820 Grad Norm 1.049757 2.11s/it\n","Train loss 868 0.903103 Grad Norm 2.207604 1.29s/it\n","Train loss 869 1.069155 Grad Norm 4.573138 1.27s/it\n","Epoch: 290\n","Train loss 870 0.675524 Grad Norm 1.157514 2.29s/it\n","Train loss 871 0.998135 Grad Norm 1.766224 1.20s/it\n","Train loss 872 0.962694 Grad Norm 1.539701 1.30s/it\n","Epoch: 291\n","Train loss 873 0.691470 Grad Norm 1.179971 2.37s/it\n","Train loss 874 0.951836 Grad Norm 2.224449 1.33s/it\n","Train loss 875 0.898494 Grad Norm 1.872971 1.33s/it\n","Epoch: 292\n","Train loss 876 0.993134 Grad Norm 2.522774 1.49s/it\n","Train loss 877 0.932449 Grad Norm 1.492279 1.37s/it\n","Train loss 878 0.943382 Grad Norm 1.784229 1.31s/it\n","Epoch: 293\n","Train loss 879 0.690371 Grad Norm 1.515809 2.16s/it\n","Train loss 880 1.009351 Grad Norm 1.589380 1.30s/it\n","Train loss 881 0.869977 Grad Norm 1.311613 1.32s/it\n","Epoch: 294\n","Train loss 882 0.656263 Grad Norm 1.882527 2.15s/it\n","Train loss 883 0.937460 Grad Norm 2.257320 1.40s/it\n","Train loss 884 0.908809 Grad Norm 1.650734 1.32s/it\n","Epoch: 295\n","Train loss 885 0.692532 Grad Norm 1.918967 2.16s/it\n","Train loss 886 0.924806 Grad Norm 1.248810 1.37s/it\n","Train loss 887 0.920783 Grad Norm 2.431262 1.31s/it\n","Epoch: 296\n","Train loss 888 0.600798 Grad Norm 0.957248 2.12s/it\n","Train loss 889 0.989841 Grad Norm 2.215153 1.28s/it\n","Train loss 890 0.904804 Grad Norm 1.105540 1.40s/it\n","Epoch: 297\n","Train loss 891 0.875491 Grad Norm 1.137905 1.52s/it\n","Train loss 892 0.925799 Grad Norm 1.396078 1.33s/it\n","Train loss 893 0.948409 Grad Norm 1.303468 1.23s/it\n","Epoch: 298\n","Train loss 894 0.975978 Grad Norm 2.829873 1.32s/it\n","Train loss 895 0.920001 Grad Norm 2.052679 1.36s/it\n","Train loss 896 0.638226 Grad Norm 1.341011 1.97s/it\n","Epoch: 299\n","Train loss 897 0.666369 Grad Norm 1.002493 2.19s/it\n","Train loss 898 0.876415 Grad Norm 1.310557 1.36s/it\n","Train loss 899 0.926654 Grad Norm 1.094028 1.31s/it\n","Epoch: 300\n","Train loss 900 0.638411 Grad Norm 0.833380 2.14s/it\n","Train loss 901 0.762435 Grad Norm 1.796412 1.34s/it\n","Train loss 902 1.110994 Grad Norm 3.236190 1.26s/it\n","Epoch: 301\n","Train loss 903 0.979597 Grad Norm 1.731186 1.47s/it\n","Train loss 904 0.852180 Grad Norm 1.871749 1.29s/it\n","Train loss 905 0.905565 Grad Norm 2.246005 1.37s/it\n","Epoch: 302\n","Train loss 906 0.907201 Grad Norm 2.119576 1.45s/it\n","Train loss 907 0.612408 Grad Norm 1.018444 1.96s/it\n","Train loss 908 0.912034 Grad Norm 1.901701 1.31s/it\n","Epoch: 303\n","Train loss 909 0.922543 Grad Norm 1.698895 1.47s/it\n","Train loss 910 0.852367 Grad Norm 1.659531 1.30s/it\n","Train loss 911 0.609784 Grad Norm 0.665101 1.95s/it\n","Epoch: 304\n","Train loss 912 0.859405 Grad Norm 1.125919 1.55s/it\n","Train loss 913 0.917166 Grad Norm 1.456172 1.31s/it\n","Train loss 914 0.605721 Grad Norm 0.826396 1.97s/it\n","Epoch: 305\n","Train loss 915 0.867947 Grad Norm 2.133712 1.43s/it\n","Train loss 916 0.648195 Grad Norm 0.687507 2.15s/it\n","Train loss 917 0.928742 Grad Norm 2.108640 1.31s/it\n","Epoch: 306\n","Train loss 918 0.921026 Grad Norm 1.523305 1.41s/it\n","Train loss 919 0.620299 Grad Norm 1.549336 1.96s/it\n","Train loss 920 0.863066 Grad Norm 2.051299 1.38s/it\n","Epoch: 307\n","Train loss 921 0.812210 Grad Norm 2.044803 1.36s/it\n","Train loss 922 0.946653 Grad Norm 1.771046 1.35s/it\n","Train loss 923 0.650556 Grad Norm 0.874506 1.95s/it\n","Epoch: 308\n","Train loss 924 0.902952 Grad Norm 1.714835 1.46s/it\n","Train loss 925 0.641793 Grad Norm 1.663000 1.94s/it\n","Train loss 926 0.864728 Grad Norm 1.422699 1.39s/it\n","Epoch: 309\n","Train loss 927 0.849507 Grad Norm 2.134849 1.44s/it\n","Train loss 928 0.896929 Grad Norm 2.016105 1.28s/it\n","Train loss 929 0.674462 Grad Norm 1.083047 1.99s/it\n","Epoch: 310\n","Train loss 930 0.826876 Grad Norm 3.259161 1.43s/it\n","Train loss 931 0.677483 Grad Norm 1.615745 1.93s/it\n","Train loss 932 0.930815 Grad Norm 2.432607 1.30s/it\n","Epoch: 311\n","Train loss 933 0.881227 Grad Norm 1.450257 1.42s/it\n","Train loss 934 0.638213 Grad Norm 1.436492 1.96s/it\n","Train loss 935 0.877039 Grad Norm 1.396917 1.34s/it\n","Epoch: 312\n","Train loss 936 0.798226 Grad Norm 1.718248 1.45s/it\n","Train loss 937 0.644781 Grad Norm 1.906255 1.98s/it\n","Train loss 938 0.924161 Grad Norm 1.421519 1.28s/it\n","Epoch: 313\n","Train loss 939 0.921690 Grad Norm 1.330528 1.42s/it\n","Train loss 940 0.617814 Grad Norm 1.270142 1.96s/it\n","Train loss 941 0.796712 Grad Norm 1.407472 1.36s/it\n","Epoch: 314\n","Train loss 942 0.840776 Grad Norm 1.331299 1.47s/it\n","Train loss 943 0.879920 Grad Norm 1.578548 1.26s/it\n","Train loss 944 0.628955 Grad Norm 0.976929 1.95s/it\n","Epoch: 315\n","Train loss 945 0.906874 Grad Norm 1.397361 1.45s/it\n","Train loss 946 0.839533 Grad Norm 1.332464 1.33s/it\n","Train loss 947 0.595860 Grad Norm 0.837882 2.00s/it\n","Epoch: 316\n","Train loss 948 0.865222 Grad Norm 1.331940 1.38s/it\n","Train loss 949 0.603941 Grad Norm 0.697071 1.99s/it\n","Train loss 950 0.846670 Grad Norm 0.744866 1.32s/it\n","Epoch: 317\n","Train loss 951 0.627344 Grad Norm 0.905588 2.14s/it\n","Train loss 952 0.819816 Grad Norm 1.406548 1.30s/it\n","Train loss 953 0.845368 Grad Norm 0.941406 1.39s/it\n","Epoch: 318\n","Train loss 954 0.882811 Grad Norm 1.592857 1.53s/it\n","Train loss 955 0.843465 Grad Norm 2.160406 1.29s/it\n","Train loss 956 0.578144 Grad Norm 1.052987 1.93s/it\n","Epoch: 319\n","Train loss 957 0.861037 Grad Norm 1.426175 1.47s/it\n","Train loss 958 0.570205 Grad Norm 1.001670 1.95s/it\n","Train loss 959 0.870164 Grad Norm 1.296691 1.58s/it\n","Epoch: 320\n"]}]},{"cell_type":"code","source":["!pip install Unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mj99hARa1K1g","executionInfo":{"status":"ok","timestamp":1676812918327,"user_tz":-540,"elapsed":4632,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"ec0536c5-7033-4dc4-948b-f798d2386d85"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Unidecode\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.3.6\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"klRf9toC1gNN","executionInfo":{"status":"ok","timestamp":1676814158136,"user_tz":-540,"elapsed":5,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"b6c1f258-9e3d-49e8-ab01-d4ed0792f0c7"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/Kss_TTS/models'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[],"metadata":{"id":"63cJO_h56YPc"},"execution_count":null,"outputs":[]}]}