{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMap9O25eWRIcj3kGUA3XCL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NT00OFcp1n2j","executionInfo":{"status":"ok","timestamp":1675490465398,"user_tz":-540,"elapsed":38995,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"b0b0074b-beab-423e-efc1-ece728e77d6b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# !pip install konlpy\n","from konlpy.tag import Okt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoCU_bMy1v1d","executionInfo":{"status":"ok","timestamp":1675490494623,"user_tz":-540,"elapsed":5613,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"3f644c01-8a26-42b7-e8cb-0d0edd00f3ea"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["FILTERS = \"([~.,!?\\\"':;)(])\"\n","\n","PAD = \"<PAD>\"\n","STD = \"<STD>\"\n","END = \"<END>\"\n","UNK = \"<UNK>\"\n","\n","# 어떤 의미도 없는 패딩 토큰\n","PAD_INDEX = 0\n","\n","# 시작 토큰\n","STD_INDEX = 1\n","\n","# 종료 토큰\n","END_INDEX = 2\n","\n","# 사전에 없는 단어\n","UNK_INDEX = 3\n","\n","MARKER = [PAD,STD,END,UNK]\n","CHANGE_FILTER = re.compile(FILTERS)\n","\n","# 데이터 분석을 통해 문장 최대 길이를 설정함\n","# 문장 최대 길이 25\n","MAX_SQEUENCE = 25 "],"metadata":{"id":"5C7Uv9AI1_FU","executionInfo":{"status":"ok","timestamp":1675499182330,"user_tz":-540,"elapsed":1004,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["\n","# 질문데이터 list, 응답데이터 list 반환하는 메서드\n","def load_data(path):\n","    # 판다스를 통해서 데이터를 불러온다.\n","    \n","    # * header=0 첫 인덱스를 0으로 시작함\n","\n","    data_df = pd.read_csv(path, header=0,encoding='cp949')\n","    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","\n","    return question, answer\n","\n","# data = 질문 데이터 list , 대답 데이터 list 모두 더한 뒤 extend\n","# data = [형태소로 띄어쓰기 된 질문 문장1,....형태소로 띄어쓰기 된 대답 문장1.....]\n","def data_tokenizer(data):\n","    # 토크나이징 해서 담을 배열 생성\n","    words = []\n","    \n","    # sentence는 문장 1개\n","    for sentence in data:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 위 필터와 같은 값들을 정규화 표현식을\n","        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n","        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","\n","        # sentence는 띄어쓰기로 형태소 구분된 문장임\n","        # 띄어쓰기된 단어 하나하나 넣어줌\n","        for word in sentence.split():\n","            words.append(word)\n","    # 토그나이징과 정규표현식을 통해 만들어진\n","    # 값들을 넘겨 준다.\n","\n","    # 형태소 단위의 단어 리스트를 반환함\n","    return [word for word in words if word]\n","\n","\n","def prepro_like_morphlized(data):\n","    morph_analyzer = Okt()\n","    result_data = list()\n","\n","     # data 는 [문장1, 문장2...]\n","     # seq는 문장1개\n","    for seq in tqdm(data):\n","\n","        # 문장의 띄어쓰기 없애고\n","        # 형태소 분석\n","        # okt().morphs 하면 \n","        # ['모바일', '게임', '은', '재밌다', '열심히', '해서', '만', '랩', '을', '찍어야지', '~', 'ㅎㅎㅎ']으로 반환\n","\n","        # \"\",join() 함수 예제\n","        # list를 string 으로\n","        # >>> time_list\n","        # ['10', '34', '17']\n","        # >>> ':'.join(time_list)\n","        # '10:34:17'\n","\n","        # morphlized_seq = 띄어쓰기로 형태소 구분된 문자열 \n","        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n","\n","        result_data.append(morphlized_seq)\n","\n","    # result_data = 띄어쓰기로 형태소 구분된 문자열의 리스트\n","    print(f'prepro_like_morphlized 메서드 결과 result_data[0]:{result_data[0]}')\n","    return result_data\n","\n","\n","def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n","    # 사전을 담을 배열 준비한다.\n","    vocabulary_list = []\n","    # 사전을 구성한 후 파일로 저장 진행한다.\n","    # 그 파일의 존재 유무를 확인한다.\n","    if not os.path.exists(vocab_path):\n","        # 이미 생성된 사전 파일이 존재하지 않으므로\n","        # 데이터를 가지고 만들어야 한다.\n","        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n","        # 데이터 파일의 존재 유무를 확인한다.\n","        if (os.path.exists(path)):\n","            # 데이터가 존재하니 판단스를 통해서\n","            # 데이터를 불러오자\n","            data_df = pd.read_csv(path, encoding='cp949')\n","            # 판다스의 데이터 프레임을 통해서\n","            # 질문과 답에 대한 열을 가져 온다.\n","            question, answer = list(data_df['Q']), list(data_df['A'])\n","\n","            # question = [문장1, 문장2...]\n","            # answer = [문장1, 문장2...]\n","\n","            if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n","                question = prepro_like_morphlized(question)\n","                answer = prepro_like_morphlized(answer)\n","            data = []\n","\n","            # 질문과 답변을 extend을\n","            # 통해서 구조가 없는 배열로 만든다.(1차원 리스트로 저장)\n","            # list.extend 예제\n","            # https://m.blog.naver.com/wideeyed/221541104629\n","\n","            data.extend(question)\n","            data.extend(answer)\n","            # 토큰나이져 처리 하는 부분이다.\n","            words = data_tokenizer(data)\n","            # 공통적인 단어에 대해서는 모두\n","            # 필요 없으므로 한개로 만들어 주기 위해서\n","            # set해주고 이것들을 리스트로 만들어 준다.\n","            words = list(set(words))\n","            # 데이터 없는 내용중에 MARKER를 사전에\n","            # 추가 하기 위해서 아래와 같이 처리 한다.\n","            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n","            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n","            # PAD = \"<PADDING>\"\n","            # STD = \"<START>\"\n","            # END = \"<END>\"\n","            # UNK = \"<UNKNWON>\"\n","            words[:0] = MARKER\n","        # 사전을 리스트로 만들었으니 이 내용을\n","        # 사전 파일을 만들어 넣는다.\n","        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n","\n","            # words는 중복없는 형태소 단위의 단어 리스트임(질문+응답 데이터 모두 포함)\n","            for word in words:\n","                vocabulary_file.write(word + '\\n')\n","        # vocaulbary_file 예시\n","        # 나\n","        # 는\n","        # 밥\n","        # 을\n","        # 좋아\n","        # 한다\n","\n","    # 사전 파일이 존재하면 여기에서\n","    # 그 파일을 불러서 배열에 넣어 준다.\n","    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n","        for line in vocabulary_file:\n","            vocabulary_list.append(line.strip())\n","\n","    # vocabulary_list는 [형태소 단위의 중복 없는 단어1,2,....]\n","\n","    # 배열에 내용을 키와 값이 있는\n","    # 딕셔너리 구조로 만든다.\n","    char2idx, idx2char = make_vocabulary(vocabulary_list)\n","    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n","\n","    # 반환 => 단어: 인덱스 , 인덱스: 단어, 총 단어의 갯수\n","    return char2idx, idx2char, len(char2idx)\n","\n","\n","def make_vocabulary(vocabulary_list):\n","    # 리스트를 키가 단어이고 값이 인덱스인\n","    # 딕셔너리를 만든다.\n","    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n","    # 리스트를 키가 인덱스이고 값이 단어인\n","    # 딕셔너리를 만든다.\n","    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n","    # 두개의 딕셔너리를 넘겨 준다.\n","\n","    # char2idx = 단어:인덱스 딕셔너리\n","    # idx2char = 인덱스:단어 딕셔너리\n","    return char2idx, idx2char\n","\n","PATH = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/chatbot_s2s/data/ChatbotData.csv'\n","VOCAB_PATH = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/chatbot_s2s/data/vocabulary.txt'\n","inputs , outputs = load_data(PATH)\n","char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"],"metadata":{"id":"pDsQ0oNf9Fw_","executionInfo":{"status":"ok","timestamp":1675499182792,"user_tz":-540,"elapsed":5,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# inputs = 질문 리스트, outputs = 대답 리스트\n","\n","inputs[1], outputs[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gY7ySat2O75b","executionInfo":{"status":"ok","timestamp":1675499182792,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"8e39d169-5f28-4f16-c048-3c2a77110861"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('1지망 학교 떨어졌어', '위로해 드립니다.')"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# char2dix 는 형태소 마다 인덱스를 부여했음\n","char2idx['나'] ,char2idx['하세요'],char2idx['어제'],char2idx['하다'] , char2idx['너는'] , char2idx['누구야'] , idx2char[13], vocab_size "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hS6Ptz5oM6_u","executionInfo":{"status":"ok","timestamp":1675499184812,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"0ad58449-4723-4d11-dd5e-fd31af9bfb44"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16114, 1931, 10104, 3615, 3863, 11004, '거니', 20705)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["char2idx['나'] ,char2idx['나는'],char2idx['어제'],char2idx['어제는'] , char2idx['먹었다'] , char2idx['먹다']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yL7crbYTWVER","executionInfo":{"status":"ok","timestamp":1675499186126,"user_tz":-540,"elapsed":7,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"faa01750-0015-463c-b7c8-7cf06828d937"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16114, 1391, 10104, 8357, 14039, 960)"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["char2idx['저'] ,char2idx['저는'],char2idx['어제는'],char2idx['보름'] , char2idx['아빠'] , char2idx['내가']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qe3l3uhsW6Cd","executionInfo":{"status":"ok","timestamp":1675499186127,"user_tz":-540,"elapsed":7,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"2175cfb6-670f-41ae-871e-cf9e120e427c"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14701, 3611, 8357, 18470, 9931, 11178)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["char2idx['<PAD>'], char2idx['<STD>'],char2idx['<END>'],char2idx['<UNK>'], idx2char[0], idx2char[1], idx2char[2], idx2char[3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKJPvXaEdrAc","executionInfo":{"status":"ok","timestamp":1675499186127,"user_tz":-540,"elapsed":5,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"b28828c6-571b-4ef8-9f3d-cdddc48581a6"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 1, 2, 3, '<PAD>', '<STD>', '<END>', '<UNK>')"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# okt가 형태소 분석을 100% 정확하게 했다고 볼 수 없다"],"metadata":{"id":"MVGYHLqCWhpq","executionInfo":{"status":"ok","timestamp":1675499186127,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["# 여기까지 dict형 형태소 사전 만듬\n","\n","# 1. 형태소:인덱스\n","# 2. 인덱스:형태소\n","# 3. 단어갯수"],"metadata":{"id":"TAW470J_ONaP"}},{"cell_type":"code","source":["MAX_SQEUENCE = 25\n","MAX_SEQUENCE = 25 "],"metadata":{"id":"-1JJMXvBic31","executionInfo":{"status":"ok","timestamp":1675499265235,"user_tz":-540,"elapsed":456,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Main문에서 \n","# enc_processing(inputs, char2idx, tokenize_as_morph=False)\n","# enc_processing(질문 리스트, 헝태소:인덱스 dict, tokenize_as_morph=False)\n","def enc_processing(value, dictionary, tokenize_as_morph=False):\n","\n","    MAX_SEQUENCE = 25 \n","  \n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다.)\n","    sequences_input_index = []\n","    # 하나의 인코딩 되는 문장의\n","    # 길이를 가지고 있다.(누적된다.)\n","    sequences_length = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    # 한줄씩 불어온다.\n","    # value는 질문 리스트 [질문 문장1, 질문 문장2,...] \n","    # sequence 는 질문 문장 1개\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 하나의 문장을 인코딩 할때\n","        # 가지고 있기 위한 배열이다.\n","        sequence_index = []\n","        # 문장을 스페이스 단위로\n","        # 자르고 있다.\n","\n","        # sequence.split() 은 ['나는', '밥을', '먹었다']\n","        for word in sequence.split():\n","            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n","            # 그 값을 가져와 sequence_index에 추가한다.\n","\n","            # *형태소 단위로 인덱스 부여한 dictionary에\n","            # 단어 단위로 잘라 key값을 찾아보고\n","            # 있으면 sequence_indxe에 인덱스 추가\n","            if dictionary.get(word) is not None:\n","\n","                # 리스트.extend(요소1개)\n","                # list = [2, 9, 3]\n","                # list.append('a')\n","                # print(list)\n","\n","                sequence_index.extend([dictionary[word]])\n","            \n","            # 없으면 \n","            # 경우 이므로 UNK를(3를) 넣어 준다.\n","            else:\n","                sequence_index.extend([dictionary[UNK]])\n","\n","        # sequence_index[0] = [34,6,21312,31223,3,123]\n","        # 없으면 '<UNK>' = 3\n","\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        \n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","\n","        # sequcences_legth => padding까지한 문장 당 번호를 붙힘\n","        # 하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0) = 0를 넣어준다.\n","        # sequence_index[0] = [34,6,21312,31223,3,123,0,0,0,0,0,0,0....] 촤대길이까지 맞쳐줌\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        \n","        # * 문장 길이가 길면 자르고\n","        #   짧으면 PAD를 넣어준다 => 학습하기 위해 문장 길이를 맞혀준다.\n","        \n","        # 인덱스화 되어 있는 값을\n","        # sequences_input_index에 넣어 준다.\n","        sequences_input_index.append(sequence_index)\n","\n","\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n","    # 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과\n","    # 그 길이를 넘겨준다.\n","    return np.asarray(sequences_input_index), sequences_length\n","\n","# Main문에서\n","# dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n","# dec_output_processing(대답 리스트, 인덱스:형태소, tokenize_as_morph=False)\n","def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n","\n","    MAX_SEQUENCE = 25 \n","\n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다)\n","    sequences_output_index = []\n","    # 하나의 디코딩 입력 되는 문장의\n","    # 길이를 가지고 있다.(누적된다)\n","    sequences_length = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","    # 한줄씩 불어온다.\n","\n","    # sequence는 대답 문장 1개\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 하나의 문장을 디코딩 할때 가지고\n","        # 있기 위한 배열이다.\n","        sequence_index = []\n","        # 디코딩 입력의 처음에는 START가 와야 하므로\n","        # 그 값을 넣어 주고 시작한다.\n","        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n","        # 값인 인덱스를 넣어 준다.\n","        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        if len(sequence_index) > MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE]\n","        # 하나의 문장에 길이를 넣어주고 있다.\n","        sequences_length.append(len(sequence_index))\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을\n","        # sequences_output_index 넣어 준다.\n","        sequences_output_index.append(sequence_index)\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n","    # 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_output_index), sequences_length\n","\n","# Main문에서\n","# dec_target_processing(outputs, char2idx, tokenize_as_morph=False)\n","# dec_target_processing(대답 리스트, 형태소:인덱스 dict, tokenize_as_morph=False)\n","def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n","\n","    MAX_SEQUENCE = 25 \n","\n","    # 인덱스 값들을 가지고 있는\n","    # 배열이다.(누적된다)\n","    sequences_target_index = []\n","    # 형태소 토크나이징 사용 유무\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","    # 한줄씩 불어온다.\n","\n","    # sequence는 대답 문장 1개\n","    for sequence in value:\n","        # FILTERS = \"([~.,!?\\\"':;)(])\"\n","        # 정규화를 사용하여 필터에 들어 있는\n","        # 값들을 \"\" 으로 치환 한다.\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        # 문장에서 스페이스 단위별로 단어를 가져와서\n","        # 딕셔너리의 값인 인덱스를 넣어 준다.\n","        # 디코딩 출력의 마지막에 END를 넣어 준다.\n","        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n","        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n","        # 그리고 END 토큰을 넣어 준다\n","        if len(sequence_index) >= MAX_SEQUENCE:\n","            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n","        else:\n","            sequence_index += [dictionary[END]]\n","        # max_sequence_length보다 문장 길이가\n","        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n","        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n","        # 인덱스화 되어 있는 값을\n","        # sequences_target_index에 넣어 준다.\n","        sequences_target_index.append(sequence_index)\n","    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n","    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n","    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n","    return np.asarray(sequences_target_index)\n","\n","# 질문 텍스트-> 벡터화 리스트, 질문 원본 인덱스 갯수 리스트(PADDING 전)\n","# <PAD> 만 추가함\n","index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)\n","\n","# 대답 텍스트-> 벡터화 리스트, 대답 원본 인덱스 갯수 리스트(PADDING 전)\n","# 대답 데이터는  [[<STD>,숫자,숫자..],[<STD>,숫자1,숫자2...]...]\n","index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n","\n","# 대답 텍스트-> [[숫자1,숫자2....<END>,<PAD>,<PAD>]]로 변형\n","index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"],"metadata":{"id":"I6DYqFQWkuCw","executionInfo":{"status":"ok","timestamp":1675499336613,"user_tz":-540,"elapsed":619,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["inputs[13], outputs[13]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwIosz0aiEAM","executionInfo":{"status":"ok","timestamp":1675499359041,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"bdc786b8-0d01-4a0d-edd0-fd2c869d4295"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('가끔은 혼자인게 좋다', '혼자를 즐기세요.')"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# '가끔은 혼자인게 좋다' => 인덱스화 \n","index_inputs[13], input_seq_len[13]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJN8MrNni0bw","executionInfo":{"status":"ok","timestamp":1675499371386,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"2cb2098c-a4dc-4f48-ae69-e5167468f90c"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 4595, 11035, 11003,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0]), 3)"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["# '혼자를 즐기세요.' => <STD> + 인덱스화 + <PAD>...\n","\n","index_outputs[13], output_seq_len[13]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iic7EYgYi3fz","executionInfo":{"status":"ok","timestamp":1675499393565,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"4a3ff559-56e5-4e40-e868-d9ce62af0ed7"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([   1, 4557, 6503,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0]), 3)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# '혼자를 즐기세요.' => 인덱스화 + <END> + <PAD>....\n","\n","index_targets[13]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6--XeBw5i8me","executionInfo":{"status":"ok","timestamp":1675499420938,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"63872014-3bee-445d-a53c-6384a5049ae9"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4557, 6503,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0])"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["data_configs = {}\n","data_configs['char2idx'] = char2idx\n","data_configs['idx2char'] = idx2char\n","data_configs['vocab_size'] = vocab_size\n","data_configs['pad_symbol'] = PAD\n","data_configs['std_symbol'] = STD\n","data_configs['end_symbol'] = END\n","data_configs['unk_symbol'] = UNK"],"metadata":{"id":"YpGBIMi6jDgt","executionInfo":{"status":"ok","timestamp":1675499567449,"user_tz":-540,"elapsed":460,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["char2idx , idx2char , vocab_size"],"metadata":{"id":"20u2iTxjkXG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_IN_PATH = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/chatbot_s2s/data/'\n","TRAIN_INPUTS = 'train_inputs.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIGS = 'data_configs.json'"],"metadata":{"id":"AiY1fdzcjnHJ","executionInfo":{"status":"ok","timestamp":1675499702747,"user_tz":-540,"elapsed":1009,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n","np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n","np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n","\n","json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"],"metadata":{"id":"BahhcSvij691","executionInfo":{"status":"ok","timestamp":1675499703439,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 전처리 끝!"],"metadata":{"id":"mAo4L7fzkMT5"}},{"cell_type":"markdown","source":["# 시퀀스 투 시퀀스 모델 만들기!"],"metadata":{"id":"yzC-8aJ-kxgL"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt"],"metadata":{"id":"21OW43emkISf","executionInfo":{"status":"ok","timestamp":1675499866203,"user_tz":-540,"elapsed":2793,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.plot(history.history['val_'+string], '')\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.legend([string, 'val_'+string])\n","    plt.show()"],"metadata":{"id":"sLusuB-qkvlo","executionInfo":{"status":"ok","timestamp":1675499903948,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["DATA_IN_PATH = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/chatbot_s2s/data/'\n","DATA_OUT_PATH = '/content/drive/MyDrive/새싹_인공지능SW교육/프로젝트/새싹_최종프로젝트/chatbot_s2s/models/'\n","TRAIN_INPUTS = 'train_inputs.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIGS = 'data_configs.json'"],"metadata":{"id":"Vwm9ADKqk5OB","executionInfo":{"status":"ok","timestamp":1675500313827,"user_tz":-540,"elapsed":477,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["SEED_NUM = 1234\n","tf.random.set_seed(SEED_NUM)"],"metadata":{"id":"SuDjWohSk-cr","executionInfo":{"status":"ok","timestamp":1675500315842,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n","index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n","index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n","prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"],"metadata":{"id":"794Xv2PVk-zN","executionInfo":{"status":"ok","timestamp":1675500315842,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["# Show length\n","print(len(index_inputs),  len(index_outputs), len(index_targets))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hYUglRMlAeZ","executionInfo":{"status":"ok","timestamp":1675500316296,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}},"outputId":"bdc3a48c-9abb-4f34-9c0e-b095564866fa"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["11823 11823 11823\n"]}]},{"cell_type":"code","source":["MODEL_NAME = 'seq2seq_kor'\n","BATCH_SIZE = 2\n","MAX_SEQUENCE = 25\n","EPOCH = 30\n","UNITS = 1024\n","EMBEDDING_DIM = 256\n","VALIDATION_SPLIT = 0.1 \n","\n","char2idx = prepro_configs['char2idx']\n","idx2char = prepro_configs['idx2char']\n","std_index = prepro_configs['std_symbol']\n","end_index = prepro_configs['end_symbol']\n","vocab_size = prepro_configs['vocab_size']"],"metadata":{"id":"qNoEinonlCSm","executionInfo":{"status":"ok","timestamp":1675500318034,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.vocab_size = vocab_size \n","        self.embedding_dim = embedding_dim          \n","        \n","        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)\n","        return output, state\n","\n","    def initialize_hidden_state(self, inp):\n","        return tf.zeros((tf.shape(inp)[0], self.enc_units))"],"metadata":{"id":"qIlI9YxMlKro","executionInfo":{"status":"ok","timestamp":1675500318034,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","        score = self.V(tf.nn.tanh(\n","            self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights"],"metadata":{"id":"lvNDcc6wlNGo","executionInfo":{"status":"ok","timestamp":1675500318673,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        \n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.vocab_size = vocab_size \n","        self.embedding_dim = embedding_dim  \n","        \n","        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc = tf.keras.layers.Dense(self.vocab_size)\n","\n","        self.attention = BahdanauAttention(self.dec_units)\n","        \n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","        x = self.embedding(x)\n","\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","        output, state = self.gru(x)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","            \n","        x = self.fc(output)\n","        \n","        return x, state, attention_weights"],"metadata":{"id":"LryCLW2dlTSy","executionInfo":{"status":"ok","timestamp":1675500318673,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n","\n","def loss(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n","\n","def accuracy(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n","    pred *= mask    \n","    acc = train_accuracy(real, pred)\n","\n","    return tf.reduce_mean(acc)"],"metadata":{"id":"Tm7WBMYclZ9l","executionInfo":{"status":"ok","timestamp":1675500318674,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["class seq2seq(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_sz, end_token_idx=2):    \n","        super(seq2seq, self).__init__()\n","        self.end_token_idx = end_token_idx\n","        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_sz) \n","        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_sz) \n","\n","    def call(self, x):\n","        inp, tar = x\n","        \n","        enc_hidden = self.encoder.initialize_hidden_state(inp)\n","        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n","\n","        dec_hidden = enc_hidden\n","\n","        predict_tokens = list()\n","        for t in range(0, tar.shape[1]):\n","            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:, t], 1), tf.float32) \n","            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n","            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))   \n","        return tf.stack(predict_tokens, axis=1)\n","    \n","    def inference(self, x):\n","        inp  = x\n","\n","        enc_hidden = self.encoder.initialize_hidden_state(inp)\n","        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n","\n","        dec_hidden = enc_hidden\n","        \n","        dec_input = tf.expand_dims([char2idx[std_index]], 1)\n","        \n","        predict_tokens = list()\n","        for t in range(0, MAX_SEQUENCE):\n","            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n","            predict_token = tf.argmax(predictions[0])\n","            \n","            if predict_token == self.end_token_idx:\n","                break\n","            \n","            predict_tokens.append(predict_token)\n","            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)   \n","            \n","        return tf.stack(predict_tokens, axis=0).numpy()"],"metadata":{"id":"mkG47A_elblE","executionInfo":{"status":"ok","timestamp":1675500319123,"user_tz":-540,"elapsed":1,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, char2idx[end_index])\n","model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])\n","#model.run_eagerly = True"],"metadata":{"id":"OMHmyZlBledM","executionInfo":{"status":"ok","timestamp":1675500320036,"user_tz":-540,"elapsed":2,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["PATH = DATA_OUT_PATH + MODEL_NAME\n","if not(os.path.isdir(PATH)):\n","        os.makedirs(os.path.join(PATH))\n","        \n","checkpoint_path = DATA_OUT_PATH + MODEL_NAME + '/weights.h5'\n","    \n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n","\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n","\n","history = model.fit([index_inputs, index_outputs], index_targets,\n","                    batch_size=BATCH_SIZE, epochs=EPOCH,\n","                    validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXC8sV1Nlf3X","outputId":"bf54dbfd-2d90-4ce7-8567-1d994bc14747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","3942/5320 [=====================>........] - ETA: 3:15 - loss: 1.2673 - accuracy: 0.8569"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kAgLxMsolh6Y"},"execution_count":null,"outputs":[]}]}